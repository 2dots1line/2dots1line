
### **`2.8_V11.0_MaintenanceWorker.md`**

---

# **V11.0 Canonical Guide: `MaintenanceWorker`**

**Document Version:** 11.0 (Headless Service Architecture)
**Purpose:** To provide a definitive specification for the `MaintenanceWorker`. This worker is the system's automated janitor and health monitor, responsible for periodic data cleanup, integrity checks, and database optimization tasks.

## **1. Core Job Responsibility & Philosophy**

The `MaintenanceWorker` ensures the long-term health, performance, and integrity of the 2dots1line platform. It runs a series of scheduled, non-urgent tasks that are vital for preventing data bloat, identifying inconsistencies, and keeping the databases performing optimally.

**Philosophy:**
*   **Scheduled & Non-Blocking:** The worker is triggered by an internal cron-style scheduler, designed to run during off-peak hours to minimize impact on production traffic.
*   **Idempotent & Safe:** Each maintenance task is designed to be safely re-run without causing issues. Operations are performed in batches to avoid long-running transactions or excessive memory usage.
*   **Comprehensive & Modular:** The worker is structured as a collection of distinct, configurable tasks. New maintenance jobs can be added easily without altering existing ones.
*   **Observability First:** The primary output of this worker is detailed logs. It reports what it checked, what it found, and what actions it took.

**Location:** `workers/maintenance-worker/`

## **2. V11.0 Architecture & Integration**

The `MaintenanceWorker` is a standalone process managed by PM2. It does not consume from a public queue but instead uses an internal scheduler (`node-cron`) to trigger its own jobs.

```
┌──────────────────────────┐
│      Scheduler (cron)    │
│  (e.g., runs nightly at  │
│         2:00 AM)         │
└────────────┬─────────────┘
             │ 1. Triggers the maintenance cycle
             ▼
┌──────────────────────────┐
│ `MaintenanceWorker`      │
│ (Singleton PM2 Process)  │
└─┬─┬─┬────────────────────┘
  │ │ │
  │ │ │ 2. Executes a series of configured tasks in sequence or parallel
  │ │ │
  │ │ └─► Task 3: Data Integrity Check ──────► Neo4j & PostgreSQL
  │ │
  │ └───► Task 2: Old Data Archiving ────────► PostgreSQL
  │
  └─────► Task 1: Stale Key Cleanup ─────────► Redis
```

## **3. Detailed Workflow & Tasks**

The worker's main process initiates a sequence of modular tasks based on their configured cron schedules.

### **Task 1: Stale Redis Key Cleanup**

*   **Trigger:** Runs hourly (`cleanup_redis_cron`).
*   **Action:**
    1.  Scans for Redis keys matching patterns like `turn_context:*` or `sse_connections:*`.
    2.  Identifies keys that have no TTL (`ttl` command returns -1) and are older than a configured threshold (e.g., `stale_key_threshold_hours`).
    3.  Logs a warning and deletes the stale key.
*   **Purpose:** Prevents memory leaks in Redis from keys that failed to expire correctly.

### **Task 2: Data Integrity Cross-Check**

*   **Trigger:** Runs daily (`integrity_check_cron`).
*   **Action:**
    1.  **Orphaned Graph Nodes:** Fetches all `userId` and `nodeId` pairs from Neo4j. For each, verifies a corresponding record exists in PostgreSQL (`memory_units`, `concepts`, etc.). Logs any discrepancies.
    2.  **Orphaned PG Records:** Scans PostgreSQL for entities that should have a graph representation and verifies they exist in Neo4j.
    3.  **Vector Store Sync:** Checks a sample of entities in PostgreSQL and confirms they have a corresponding vector in Weaviate.
*   **Purpose:** Detects data drift or inconsistencies between the different persistence layers, which can result from partially failed transactions.

### **Task 3: Database Optimization**

*   **Trigger:** Runs weekly (`db_optimization_cron`).
*   **Action:**
    1.  Connects to the PostgreSQL database.
    2.  Executes `VACUUM ANALYZE` on high-traffic tables like `conversation_messages`, `cards`, and `memory_units`.
    3.  Could be extended to run `REINDEX` on specific database indexes if performance degradation is detected.
*   **Purpose:** Maintains PostgreSQL query planner performance and reclaims disk space.

### **Task 4: Old Data Archiving**

*   **Trigger:** Runs monthly (`archiving_cron`).
*   **Action:**
    1.  Scans for `conversations` older than a configured threshold (e.g., `archive_conversations_after_days: 730`).
    2.  Changes the `status` of the conversation to `'archived'`.
    3.  Optionally, could move associated `conversation_messages` to a separate, cheaper storage or an archive table.
*   **Purpose:** Manages the size of primary tables to keep queries fast, while retaining data for potential future analysis.

## **4. Dependencies & Collaborators**

| Component Name | Type | Role & Responsibility |
| :--- | :--- | :--- |
| **All Database Repositories/Clients** | **Key Dependency** | The worker requires privileged access to PostgreSQL, Neo4j, Redis, and Weaviate to perform its tasks. |
| **`node-cron`** | Library | Used internally to schedule the execution of its own maintenance tasks. |
| **Logger** | Service | A critical dependency for providing detailed reports on all actions taken. |

## **5. Configuration & Environment Variables**

```dotenv
# workers/maintenance-worker/.env

# --- Task Enablement ---
ENABLE_REDIS_CLEANUP_TASK=true
ENABLE_INTEGRITY_CHECK_TASK=true
ENABLE_DB_OPTIMIZATION_TASK=true
ENABLE_ARCHIVING_TASK=false # Default to false for safety

# --- Cron Schedules ---
CLEANUP_REDIS_CRON="0 * * * *" # Every hour
INTEGRITY_CHECK_CRON="0 2 * * *" # Daily at 2:00 AM
DB_OPTIMIZATION_CRON="0 3 * * 1" # Weekly on Monday at 3:00 AM
ARCHIVING_CRON="0 4 1 * *" # Monthly on the 1st at 4:00 AM

# --- Task-Specific Configs ---
STALE_REDIS_KEY_THRESHOLD_HOURS=24
ARCHIVE_CONVERSATIONS_AFTER_DAYS=730
INTEGRITY_CHECK_BATCH_SIZE=1000
```

## **6. Scalability & Performance**

*   **Singleton Process:** The `MaintenanceWorker` should always run as a single instance (`instances: 1` in PM2) to avoid race conditions where two workers try to clean up or archive the same data simultaneously.
*   **Batch Processing:** All database-intensive tasks (integrity checks, archiving) must be performed in batches (e.g., 1,000 records at a time) to avoid holding database locks for extended periods and to keep memory usage low.
*   **Off-Peak Execution:** The cron schedules must be configured for off-peak hours to minimize any potential impact on user-facing performance.

# Note to Cursor

Of course. As Tech Lead, I've reviewed the provided `MaintenanceWorker.ts` implementation.

This is a solid effort and demonstrates good modularity and a clear understanding of basic cleanup tasks. However, it significantly diverges from the V11.0 specification by omitting the most critical, architecturally-driven tasks and introducing logic that belongs elsewhere in our system.

Let's break it down.

### **High-Level Architectural Mismatch**

The primary issue is that this implementation focuses on application-level data cleanup while **completely ignoring the crucial infrastructure-level maintenance tasks** defined in the `6.2_V11.0_MaintenanceWorker.md` spec.

**The following required tasks are MISSING:**
1.  **Stale Redis Key Cleanup:** There is no check for orphaned Redis keys, which is a key task for preventing memory leaks.
2.  **Data Integrity Cross-Check:** This is the **most critical omission**. The worker does not verify consistency between PostgreSQL, Neo4j, and Weaviate. This is the single most important job for this worker in our multi-database architecture.
3.  **Database Optimization:** There is no `VACUUM ANALYZE` or other database optimization task, which is essential for long-term PostgreSQL performance.

Additionally, one of the implemented tasks is architecturally misplaced.

### **Task-by-Task Analysis of the Current Implementation**

1.  **`cleanupExpiredSessions()`**
    *   **Verdict:** ✅ **Good.** This is a simple, valid maintenance task. No notes.

2.  **`archiveOldConversations()`**
    *   **Verdict:** ⚠️ **Needs Correction.** The logic is sound, but the hardcoded 30-day threshold is a **critical business logic error**. Our spec defines a much longer period (e.g., 730 days) to ensure the `InsightEngine` has sufficient long-term data. This value **must** be driven by a configuration variable.

3.  **`cleanupOrphanedMediaItems()`**
    *   **Verdict:** ⚠️ **Incomplete.** The concept is excellent, but the implementation is dangerous. It deletes the database record without deleting the actual file from storage (e.g., S3, local disk). This will lead to orphaned files and unnecessary storage costs. A production-ready implementation must include a call to a storage service to delete the file itself.

4.  **`updateUserActivityStats()`**
    *   **Verdict:** ❌ **Architecturally Incorrect.** This task is a batch job that updates a user's `last_active_at` timestamp. This is the wrong approach. The `last_active_at` field should be updated in **real-time** by a middleware in the `api-gateway` on every authenticated request. A daily batch job creates data lag and is an inefficient way to track real-time activity. **This task should be removed from the MaintenanceWorker entirely** and implemented in the API Gateway.

5.  **`cleanupTemporaryFiles()`**
    *   **Verdict:** ⚠️ **Incomplete.** Similar to the orphaned media cleanup, this is a good idea but is not production-ready without the logic to delete the file from the backing storage provider.

---

### **What to Keep vs. What to Correct**

| Component | Keep or Refactor? | Reason & Action Required |
| :--- | :--- | :--- |
| **Cron Scheduling** | Keep | The `node-cron` approach is correct for this worker. |
| **`isRunning` Lock** | Keep | A simple and effective mechanism for a singleton worker. |
| `cleanupExpiredSessions` | Keep | A valid and well-implemented task. |
| `archiveOldConversations` | **Refactor** | Keep the logic, but **must** use a configurable threshold from `process.env` and default to a long-term value (e.g., 730 days). |
| `cleanupOrphanedMediaItems` | **Refactor** | Keep the concept, but **must** add the logic to delete the corresponding file from the storage service. |
| `updateUserActivityStats` | **Remove** | **Remove this entire method.** Create a new ticket to implement this functionality as middleware in the `api-gateway`. |
| `cleanupTemporaryFiles` | **Refactor** | Same as `cleanupOrphanedMediaItems`—**must** add file deletion from storage. |

### **Corrected V11.0 `MaintenanceWorker.ts` Specification**

Here is a corrected and complete implementation that aligns with our V11.0 architecture. It includes the required tasks and refactors the useful ones from the original submission.

```typescript
/**
 * MaintenanceWorker.ts
 * V11.0 CORRECT IMPLEMENTATION
 * Worker responsible for data hygiene, integrity, and infrastructure optimization.
 */

import { DatabaseService } from '@2dots1line/database';
import * as cron from 'node-cron';

// V11.0: All thresholds and schedules are configuration-driven.
const CONFIG = {
  ENABLE_REDIS_CLEANUP: process.env.ENABLE_REDIS_CLEANUP_TASK === 'true',
  ENABLE_INTEGRITY_CHECK: process.env.ENABLE_INTEGRITY_CHECK_TASK === 'true',
  ENABLE_DB_OPTIMIZATION: process.env.ENABLE_DB_OPTIMIZATION_TASK === 'true',
  ENABLE_ARCHIVING: process.env.ENABLE_ARCHIVING_TASK === 'true', // Default to false for safety

  REDIS_CLEANUP_CRON: process.env.CLEANUP_REDIS_CRON || '0 * * * *', // Hourly
  INTEGRITY_CHECK_CRON: process.env.INTEGRITY_CHECK_CRON || '0 2 * * *', // Daily at 2 AM
  DB_OPTIMIZATION_CRON: process.env.DB_OPTIMIZATION_CRON || '0 3 * * 1', // Weekly on Monday at 3 AM
  ARCHIVING_CRON: process.env.ARCHIVING_CRON || '0 4 1 * *', // Monthly on the 1st at 4 AM

  ARCHIVE_CONVERSATIONS_AFTER_DAYS: parseInt(process.env.ARCHIVE_CONVERSATIONS_AFTER_DAYS || '730', 10),
};

export class MaintenanceWorker {
  private dbService: DatabaseService;
  private isRunning: boolean = false;

  constructor() {
    this.dbService = DatabaseService.getInstance();
  }

  public initialize(): void {
    console.log('[MaintenanceWorker] Initializing...');
    
    // Schedule the main maintenance cycle
    cron.schedule(CONFIG.INTEGRITY_CHECK_CRON, () => {
      if (!this.isRunning) {
        this.runMaintenanceCycle();
      }
    });

    console.log(`[MaintenanceWorker] Main cycle scheduled with cron: ${CONFIG.INTEGRITY_CHECK_CRON}`);
  }

  public async runMaintenanceCycle(): Promise<void> {
    if (this.isRunning) {
      console.log('[MaintenanceWorker] Maintenance cycle already running, skipping.');
      return;
    }

    this.isRunning = true;
    console.log('[MaintenanceWorker] Starting maintenance cycle...');

    const tasks = [
      // REQUIRED TASKS FROM V11.0 SPEC
      { name: 'Cleanup Stale Redis Keys', fn: () => this.cleanupStaleRedisKeys(), enabled: CONFIG.ENABLE_REDIS_CLEANUP },
      { name: 'Run Data Integrity Check', fn: () => this.runDataIntegrityCheck(), enabled: CONFIG.ENABLE_INTEGRITY_CHECK },
      { name: 'Run Database Optimization', fn: () => this.runDatabaseOptimization(), enabled: CONFIG.ENABLE_DB_OPTIMIZATION },
      
      // REFACTORED TASKS
      { name: 'Cleanup Expired Sessions', fn: () => this.cleanupExpiredSessions(), enabled: true },
      { name: 'Archive Old Conversations', fn: () => this.archiveOldConversations(), enabled: CONFIG.ENABLE_ARCHIVING },
      // { name: 'Cleanup Orphaned Media Items', fn: () => this.cleanupOrphanedMediaItems(), enabled: true }, // Uncomment when storage deletion is implemented
    ];

    for (const task of tasks) {
      if (task.enabled) {
        try {
          console.log(`[MaintenanceWorker] Running: ${task.name}`);
          await task.fn();
          console.log(`[MaintenanceWorker] ✅ Completed: ${task.name}`);
        } catch (error) {
          console.error(`[MaintenanceWorker] ❌ Failed: ${task.name}`, error);
        }
      } else {
        console.log(`[MaintenanceWorker] ⏩ Skipping disabled task: ${task.name}`);
      }
    }

    console.log('[MaintenanceWorker] Maintenance cycle finished.');
    this.isRunning = false;
  }

  // --- REQUIRED TASKS ---

  private async cleanupStaleRedisKeys(): Promise<void> {
    // This is a simplified example. A production version would use SCAN to avoid blocking.
    const stream = this.dbService.redis.scanStream({ match: 'turn_context:*' });
    let staleKeysFound = 0;
    for await (const keys of stream) {
      for (const key of keys) {
        const ttl = await this.dbService.redis.ttl(key);
        if (ttl === -1) { // -1 means no TTL
          console.warn(`[MaintenanceWorker] Found stale Redis key with no TTL: ${key}. Deleting.`);
          await this.dbService.redis.del(key);
          staleKeysFound++;
        }
      }
    }
    console.log(`[MaintenanceWorker] Deleted ${staleKeysFound} stale Redis keys.`);
  }

  private async runDataIntegrityCheck(): Promise<void> {
    // Example: Check for orphaned concepts in PostgreSQL
    const conceptsInPg = await this.dbService.prisma.concepts.findMany({ select: { concept_id: true } });
    const conceptIdsInPg = new Set(conceptsInPg.map(c => c.concept_id));
    
    const neo4jSession = this.dbService.neo4j.session();
    try {
      const result = await neo4jSession.run('MATCH (c:Concept) RETURN c.conceptId AS conceptId');
      const conceptIdsInNeo4j = new Set(result.records.map(r => r.get('conceptId')));

      for (const pgId of conceptIdsInPg) {
        if (!conceptIdsInNeo4j.has(pgId)) {
          console.error(`[Data Integrity Error] Concept ID ${pgId} exists in PostgreSQL but NOT in Neo4j.`);
        }
      }
      console.log(`[MaintenanceWorker] Data integrity check for Concepts completed. Checked ${conceptIdsInPg.size} records.`);
    } finally {
      await neo4jSession.close();
    }
    // This would be expanded for all critical entities and databases.
  }
  
  private async runDatabaseOptimization(): Promise<void> {
    console.log('[MaintenanceWorker] Starting database optimization (VACUUM ANALYZE)...');
    await this.dbService.prisma.$executeRawUnsafe(`VACUUM ANALYZE conversations;`);
    await this.dbService.prisma.$executeRawUnsafe(`VACUUM ANALYZE conversation_messages;`);
    await this.dbService.prisma.$executeRawUnsafe(`VACUUM ANALYZE memory_units;`);
    console.log('[MaintenanceWorker] Database optimization complete.');
  }

  // --- REFACTORED TASKS ---

  private async cleanupExpiredSessions(): Promise<void> {
    const result = await this.dbService.prisma.user_sessions.deleteMany({
      where: { expires_at: { lt: new Date() } },
    });
    console.log(`[MaintenanceWorker] Deleted ${result.count} expired sessions.`);
  }

  private async archiveOldConversations(): Promise<void> {
    const archiveDate = new Date();
    archiveDate.setDate(archiveDate.getDate() - CONFIG.ARCHIVE_CONVERSATIONS_AFTER_DAYS);

    const result = await this.dbService.prisma.conversations.updateMany({
      where: {
        status: 'processed',
        ended_at: { lt: archiveDate },
      },
      data: { status: 'archived' },
    });
    console.log(`[MaintenanceWorker] Archived ${result.count} conversations older than ${CONFIG.ARCHIVE_CONVERSATIONS_AFTER_DAYS} days.`);
  }
  
  // ... other methods ...
}
```