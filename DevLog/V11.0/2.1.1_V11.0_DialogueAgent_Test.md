# **V11.0 DialogueAgent Comprehensive Testing Plan**

**Document Version:** 11.0  
**Purpose:** Ultra-granular testing plan for DialogueAgent conversation processing and hybrid retrieval workflow with specific classes, methods, database operations, and verification commands.

## **Detailed Process Flow with Exact Components**

```
USER MESSAGE → ConversationController.postMessage() → 
DialogueAgent.processTurn() → PromptBuilder.buildPrompt() → 
LLMChatTool.execute() → [DECISION BRANCH] →
  RESPOND_DIRECTLY: Return response immediately
  QUERY_MEMORY: HybridRetrievalTool.execute() → 6-Stage Pipeline →
    Stage 1: Key Phrase Processing →
    Stage 2: Weaviate Semantic Grounding →
    Stage 3: Neo4j Graph Traversal →
    Stage 4: PostgreSQL Pre-Hydration →
    Stage 5: Entity Scoring & Prioritization →
    Stage 6: Full Content Hydration →
  Second LLM Call with augmented context → Final Response
```

---

## **TESTING SCOPE & INFRASTRUCTURE VERIFICATION**

### **✅ PORT CONFLICT VERIFICATION COMPLETE**
All four containerized databases running correctly with no conflicts:
- **PostgreSQL**: Port 5433 ✅ (Docker container)
- **Neo4j**: Ports 7474/7688 ✅ (Docker container) 
- **Weaviate**: Port 8080 ✅ (Docker container)
- **Redis**: Port 6379 ✅ (Docker container, unified per Solution 1)

### **✅ EXISTING DATA ANALYSIS**
**Available Test Data**:
- **Users**: `dev-user-123`, `debug-user`, `d04766df-17e6-4a47-87ce-9c549827f54f` 
- **Conversations**: 5 existing conversations, mostly in 'active' status
- **Messages**: Existing conversation messages for testing context
- **Memory Units**: 0 (will be created during testing)
- **Concepts**: 0 (will be created during testing)
- **Neo4j**: Empty graph (will be populated during testing)
- **Weaviate**: Schema configured, no data yet

---

## **STEP 1: API Gateway Request Processing**

### **1A: HTTP Request Reception & Validation**
**What happens**: Express.js router receives POST to `/api/v1/conversations/messages`
**File**: `apps/api-gateway/src/controllers/conversation.controller.ts`
**Method**: `ConversationController.postMessage()`
**Expected**: Request validation and user authentication

**Verification Commands**:
```bash
# Test: Send valid request to API Gateway
curl -v -X POST http://localhost:3001/api/v1/conversations/messages \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer dev-token" \
  -d '{"message": "Hello, I want to test the hybrid retrieval system. Can you help me understand how memory retrieval works?", "conversation_id": "test-hybrid-retrieval-1"}'

# Expected: HTTP 200 with response or clear error with stack trace
```

**❌ Can test without API key**: Yes - HTTP routing and validation happen before LLM calls

**🔴 EXPECTED RESULT**: HTTP 500 due to LLM geo-restriction (based on previous testing)

### **1B: User Authentication & Context Loading**
**What happens**: Middleware extracts userId from JWT token
**File**: `apps/api-gateway/src/middleware/auth.middleware.ts`
**Expected**: `req.user.id = "dev-user-123"` set on request object

**Verification Commands**:
```bash
# Test: Verify dev user exists and has required context
docker exec postgres-2d1l psql -U danniwang -d twodots1line \
  -c "SELECT user_id, name, memory_profile IS NOT NULL as has_memory_profile, knowledge_graph_schema IS NOT NULL as has_kg_schema FROM users WHERE user_id = 'dev-user-123';"

# Expected: User found with memory_profile and knowledge_graph_schema
```

**❌ Can test without API key**: Yes - authentication happens before LLM processing

**✅ EXPECTED RESULT**: User exists with context data

---

## **STEP 2: Conversation Repository Operations**

### **2A: Conversation Creation/Retrieval**
**What happens**: `ConversationController.postMessage()` finds or creates conversation
**File**: `packages/database/src/repositories/ConversationRepository.ts`
**Method**: `findById()` or `create()`
**Expected Database Operation**: PostgreSQL INSERT or SELECT on `conversations` table

**Verification Commands**:
```bash
# Test: Check if conversation exists or gets created
TEST_CONV_ID="test-hybrid-retrieval-1"
echo "Before API call - checking if conversation exists:"
docker exec postgres-2d1l psql -U danniwang -d twodots1line \
  -c "SELECT id, user_id, status, start_time FROM conversations WHERE id = '${TEST_CONV_ID}';"

# Manual conversation creation for isolated testing
docker exec postgres-2d1l psql -U danniwang -d twodots1line \
  -c "INSERT INTO conversations (id, user_id, status, start_time, title) VALUES ('${TEST_CONV_ID}', 'dev-user-123', 'active', NOW(), 'Hybrid Retrieval Test') ON CONFLICT DO NOTHING;"

echo "After manual creation:"
docker exec postgres-2d1l psql -U danniwang -d twodots1line \
  -c "SELECT id, user_id, status, start_time, title FROM conversations WHERE id = '${TEST_CONV_ID}';"
```

**❌ Can test without API key**: Yes - database operations happen before LLM calls

**✅ EXPECTED RESULT**: Conversation created or found successfully

### **2B: User Message Logging**
**What happens**: `ConversationRepository.addMessage()` logs user message
**Method**: `addMessage(conversationId, 'user', messageContent)`
**Database Table**: `conversation_messages`
**Expected Fields**: conversation_id, role='user', content, timestamp

**Verification Commands**:
```bash
# Test: Verify user message gets logged (independent of LLM)
docker exec postgres-2d1l psql -U danniwang -d twodots1line \
  -c "SELECT conversation_id, role, content, timestamp FROM conversation_messages WHERE conversation_id = 'test-hybrid-retrieval-1' AND role = 'user' ORDER BY timestamp DESC LIMIT 1;"

# Expected: User message logged with correct content
```

**❌ Can test without API key**: Yes - message logging is independent of LLM processing

**🔴 EXPECTED RESULT**: No message logged if transaction rollback occurs on LLM failure

---

## **STEP 3: DialogueAgent Initialization & Dependency Injection**

### **3A: DialogueAgent Dependency Verification**
**What happens**: ConversationController uses injected DialogueAgent
**File**: `services/dialogue-service/src/DialogueAgent.ts`
**Constructor Dependencies**:
- `configService: ConfigService`
- `conversationRepository: ConversationRepository`
- `redisClient: Redis`
- `promptBuilder: PromptBuilder`
- `hybridRetrievalTool: HybridRetrievalTool`
- `llmChatTool: LLMChatTool`

**Verification Commands**:
```bash
# Test: Check DialogueAgent initialization logs
echo "Checking API Gateway startup logs for DialogueAgent initialization:"
tail -50 ~/.pm2/logs/api-gateway-out-*.log | grep -i "dialogueAgent\|initialized" | tail -5 || echo "No initialization logs found"

# Check tool registry configuration
cat config/tools_config.json | grep -A 10 -B 10 "HybridRetrievalTool" || echo "Tool config not found"
```

**❌ Can test without API key**: Yes - dependency injection happens during app startup

**✅ EXPECTED RESULT**: Dependencies inject correctly

### **3B: DialogueAgent.processTurn() Method Entry**
**What happens**: Controller calls `dialogueAgent.processTurn(input)`
**File**: `services/dialogue-service/src/DialogueAgent.ts`
**Method**: `processTurn({userId, conversationId, currentMessageText})`
**Expected**: Execution ID generation and method entry logging

**Verification Commands**:
```bash
# Test: Monitor DialogueAgent method entry (without full execution)
echo "Testing DialogueAgent method signature and entry point..."
# This requires code inspection rather than runtime testing without API key
grep -A 10 "processTurn" services/dialogue-service/src/DialogueAgent.ts | head -15
```

**❌ Can test without API key**: Partial - method signature and entry logging, but full execution blocked by LLM

**✅ EXPECTED RESULT**: Method exists with correct signature

---

## **STEP 4: PromptBuilder Context Assembly (TESTABLE WITHOUT API KEY)**

### **4A: PromptBuilder.buildPrompt() Data Fetching**
**What happens**: PromptBuilder fetches all required context data
**File**: `services/dialogue-service/src/PromptBuilder.ts`
**Method**: `buildPrompt(input: PromptBuildInput)`
**Data Sources**:
- UserRepository.findUserByIdWithContext()
- ConversationRepository.getMostRecentMessages()
- ConversationRepository.getRecentImportantConversationSummaries()
- Redis.get(`turn_context:${conversationId}`)

**Verification Commands**:
```bash
# Test: Verify all data sources return valid data for PromptBuilder
echo "1. User context data:"
docker exec postgres-2d1l psql -U danniwang -d twodots1line \
  -c "SELECT user_id, LENGTH(memory_profile) as memory_profile_length, LENGTH(knowledge_graph_schema) as kg_schema_length, next_conversation_context_package IS NOT NULL as has_next_context FROM users WHERE user_id = 'dev-user-123';"

echo "2. Recent conversation messages:"
docker exec postgres-2d1l psql -U danniwang -d twodots1line \
  -c "SELECT conversation_id, role, LENGTH(content) as content_length, timestamp FROM conversation_messages WHERE conversation_id = 'test-hybrid-retrieval-1' ORDER BY timestamp DESC LIMIT 3;"

echo "3. Recent important conversation summaries:"
docker exec postgres-2d1l psql -U danniwang -d twodots1line \
  -c "SELECT id, importance_score, LENGTH(context_summary) as summary_length FROM conversations WHERE user_id = 'dev-user-123' AND importance_score >= 1 ORDER BY start_time DESC LIMIT 3;"

echo "4. Redis turn context (should be empty for new conversation):"
docker exec redis-2d1l redis-cli GET "turn_context:test-hybrid-retrieval-1" || echo "No turn context found (expected for new conversation)"
```

**✅ Can test completely without API key**: Yes - all data fetching is independent of LLM

**✅ EXPECTED RESULT**: All data sources return valid data for prompt assembly

### **4B: Template Loading and Component Assembly**
**What happens**: PromptBuilder loads templates and assembles prompt components
**Expected Templates**:
- `preamble`
- `system_identity_template` 
- `response_format_block`
- `dialogue_agent_instructions`
- `CoreIdentity` configuration

**Verification Commands**:
```bash
# Test: Verify all required templates exist in ConfigService
echo "Checking template files:"
ls -la config/ | grep -E "(template|prompt|identity)" || echo "Template files not found in config/"

echo "Checking specific config files for DialogueAgent:"
cat config/core_identity.json | head -10 || echo "core_identity.json not found"
cat config/templates.json | head -20 || echo "templates.json not found"

echo "Checking operational parameters:"
cat config/operational_parameters.json | grep -E "(conversation|timeout)" || echo "operational_parameters.json incomplete"
```

**✅ Can test completely without API key**: Yes - template loading is configuration-only

**✅ EXPECTED RESULT**: All templates and configurations load successfully

---

## **STEP 5: LLM Processing Decision Point (REQUIRES API KEY)**

### **5A: LLMChatTool.execute() Initial Call**
**What happens**: DialogueAgent calls LLMChatTool for "Single Synthesis Call"
**File**: `packages/tools/src/ai/LLMChatTool.ts`
**Method**: `execute({systemPrompt, userPrompt, history, modelConfig})`
**Expected**: Google Gemini API call with structured conversation

**⚠️ Requires API key**: This step will fail without valid GOOGLE_API_KEY

**Verification Commands**:
```bash
# Test: Check LLM tool configuration and API key
echo "LLM configuration check:"
echo "GOOGLE_API_KEY present: ${GOOGLE_API_KEY:+YES}" || echo "GOOGLE_API_KEY: NOT SET"
grep -r "GOOGLE_API_KEY\|gemini" packages/tools/src/ai/ | head -3 || echo "LLM config not found"

# Check if we can test LLM tool offline
node -e "
const { LLMChatTool } = require('./packages/tools/src/ai/LLMChatTool');
console.log('LLM Tool available:', typeof LLMChatTool);
" 2>/dev/null || echo "LLM Tool not accessible from CLI"
```

**🔴 EXPECTED RESULT**: Google Gemini geo-restriction error - `User location is not supported for the API use`

### **5B: Response Decision Parsing**
**What happens**: DialogueAgent parses LLM response for decision routing
**Expected JSON Structure**:
```json
{
  "response_plan": {
    "decision": "respond_directly" | "query_memory",
    "direct_response_text": "...",
    "key_phrases_for_retrieval": ["phrase1", "phrase2"] | null
  },
  "turn_context_package": {...},
  "ui_actions": [...]
}
```

**⚠️ Requires API key**: Cannot test without LLM response

**🔴 EXPECTED RESULT**: No response due to API geo-restriction

---

## **STEP 6: Hybrid Retrieval Testing (PARTIALLY TESTABLE WITHOUT API KEY)**

**CRITICAL**: This is the core focus of our testing plan. We can test most of the 6-stage pipeline independently of LLM decisions.

### **6A: Test Setup - Create Sample Memory Data for Retrieval**
**Purpose**: Populate databases with test data to enable hybrid retrieval testing
**Components**: Create memory units, concepts, and vector embeddings

**Test Data Creation Commands**:
```bash
# Create comprehensive test memory data based on provided conversation sample
echo "=== Creating Test Memory Data for Hybrid Retrieval ==="

# 1. Create sample memory units about Charles's research dilemma (using correct schema)
docker exec postgres-2d1l psql -U danniwang -d twodots1line -c "
INSERT INTO memory_units (muid, user_id, title, content, importance_score, creation_ts, last_modified_ts, source_conversation_id) VALUES 
('mu_charles_columbia_research', 'dev-user-123', 'Columbia Research Position Conflict', 'Charles works at Columbia University as a researcher but has a fundamental conflict with his boss regarding paper direction. The disagreement is irreconcilable and relates to his research focus on autism, which was influenced by his son ASD diagnosis.', 8.0, NOW(), NOW(), 'test-hybrid-retrieval-1'),
('mu_charles_health_issues', 'dev-user-123', 'Health Problems and Traditional Medicine', 'Charles experiences chronic neck and shoulder pain with poor sleep quality. Doctors cannot find the root cause except as depression symptoms. He wants to seek help from Traditional Chinese Medicine practitioners for holistic treatment.', 7.0, NOW(), NOW(), 'test-hybrid-retrieval-1'),
('mu_charles_china_opportunity', 'dev-user-123', 'China University Opportunities', 'Charles has reached out to 2 universities in China willing to provide funding and resources for starting his own lab. He wants more autonomy and a relaxed working environment to pursue his autism research.', 6.0, NOW(), NOW(), 'test-hybrid-retrieval-1'),
('mu_charles_family_situation', 'dev-user-123', 'Family and Children Context', 'Charles has sent his 2 boys to local school in Changchun. His older son (9 years old) has been diagnosed with borderline ASD which deeply impacted his research direction in bio-statistics focusing on autism and protein targets.', 9.0, NOW(), NOW(), 'test-hybrid-retrieval-1'),
('mu_charles_green_card_concern', 'dev-user-123', 'Green Card and Immigration Status', 'Charles is worried about his green card status if he quits his Columbia position to return to China. This is one of his major concerns in making the decision to follow his heart.', 5.0, NOW(), NOW(), 'test-hybrid-retrieval-1')
ON CONFLICT (muid) DO NOTHING;
"

# 2. Create related concepts (using correct schema)
docker exec postgres-2d1l psql -U danniwang -d twodots1line -c "
INSERT INTO concepts (concept_id, user_id, name, type, description, salience, status, created_at, last_updated_ts) VALUES 
('concept_autism_research', 'dev-user-123', 'Autism Research', 'research_domain', 'Bio-statistical research focused on discovering links between protein targets and autistic syndromes', 0.9, 'active', NOW(), NOW()),
('concept_traditional_chinese_medicine', 'dev-user-123', 'Traditional Chinese Medicine', 'treatment_approach', 'Holistic medical approach that views body and mind as interconnected system for treating chronic conditions', 0.7, 'active', NOW(), NOW()),
('concept_work_life_balance', 'dev-user-123', 'Work-Life Balance', 'life_philosophy', 'Priority of health and family well-being over career advancement and geographic location decisions', 0.8, 'active', NOW(), NOW()),
('concept_research_autonomy', 'dev-user-123', 'Research Autonomy', 'career_goal', 'Desire to start independent lab with funding and resources to pursue meaningful research directions', 0.6, 'active', NOW(), NOW()),
('concept_cross_cultural_decision', 'dev-user-123', 'Cross-Cultural Life Decisions', 'life_transition', 'Complex decisions involving immigration status, cultural identity, and geographic relocation', 0.5, 'active', NOW(), NOW())
ON CONFLICT (concept_id) DO NOTHING;
"

echo "✅ Test memory data created successfully"
```

**✅ Can test completely without API key**: Yes - direct database population

### **6B: Stage 1 - Key Phrase Processing (TESTABLE WITHOUT API KEY)**
**What happens**: HybridRetrievalTool processes input key phrases
**File**: `packages/tools/src/retrieval/HybridRetrievalTool.ts`
**Method**: `processKeyPhrases(keyPhrases, context)`
**Logic**: Filtering, deduplication, trimming to 5 phrases max

**Manual Test Commands**:
```bash
# Test: Simulate key phrase processing logic manually
echo "=== Testing Key Phrase Processing Stage ==="

# Simulate input key phrases from Charles's conversation
TEST_PHRASES=("Columbia University research" "autism protein targets" "Traditional Chinese Medicine" "green card immigration" "work autonomy China")

echo "Input phrases: ${TEST_PHRASES[@]}"
echo "Expected processing: filter empty, deduplicate, trim to 100 chars, limit to 5"

# Test the actual HybridRetrievalTool if possible
node -e "
const phrases = ['Columbia University research', 'autism protein targets', 'Traditional Chinese Medicine', 'green card immigration', 'work autonomy China', 'health depression neck pain'];
console.log('Original phrases:', phrases.length);
const processed = phrases.filter(p => p && p.trim().length > 0).map(p => p.trim().substring(0, 100)).slice(0, 5);
const deduplicated = [...new Set(processed)];
console.log('Processed phrases:', deduplicated);
"
```

**✅ Can test completely without API key**: Yes - pure data processing logic

**✅ EXPECTED RESULT**: Key phrases processed with filtering and deduplication

### **6C: Stage 2 - Weaviate Semantic Grounding (TESTABLE WITHOUT API KEY)**
**What happens**: Query Weaviate for semantically similar content
**File**: `packages/tools/src/retrieval/HybridRetrievalTool.ts`
**Method**: `semanticGrounding(phrases, userId, context)`
**Database**: Weaviate UserKnowledgeItem class
**Query**: `withNearText({ concepts: [phrase] })`

**Test Commands**:
```bash
# Test: Direct Weaviate query to verify semantic grounding capability
echo "=== Testing Weaviate Semantic Grounding ==="

# Check current Weaviate data
echo "Current UserKnowledgeItem objects:"
curl -s -X GET "http://localhost:8080/v1/objects?class=UserKnowledgeItem&limit=5" | jq '.objects | length' || echo "Weaviate query failed"

# Since we likely have no vector data yet, create some test embeddings
echo "Creating test vector embeddings for our memory units..."
curl -X POST "http://localhost:8080/v1/objects" \
  -H "Content-Type: application/json" \
  -d '{
    "class": "UserKnowledgeItem",
    "properties": {
      "externalId": "mu_charles_columbia_research",
      "userId": "dev-user-123",
      "sourceEntityType": "MemoryUnit",
      "sourceEntityId": "mu_charles_columbia_research",
      "textContent": "Charles works at Columbia University as a researcher but has a fundamental conflict with his boss regarding paper direction. The disagreement is irreconcilable and relates to his research focus on autism.",
      "title": "Columbia Research Position Conflict",
      "embeddingModelVersion": "test-v1",
      "createdAt": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'",
      "importanceScore": 8,
      "tags": ["research", "columbia", "autism", "conflict"]
    }
  }' || echo "Failed to create test vector embedding"

# Test semantic search
echo "Testing semantic search with key phrase:"
curl -X POST "http://localhost:8080/v1/graphql" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "{ Get { UserKnowledgeItem(where: {path: [\"userId\"], operator: Equal, valueString: \"dev-user-123\"}, nearText: {concepts: [\"autism research Columbia\"]}, limit: 3) { externalId sourceEntityType _additional { distance } } } }"
  }' | jq '.' || echo "Semantic search failed"
```

**⚠️ Requires API key for embeddings**: Weaviate semantic search requires embeddings, which need Google Gemini API

**🔴 EXPECTED RESULT**: Limited functionality - can test object creation but not semantic search due to embedding geo-restrictions

### **6D: Stage 3 - Neo4j Graph Traversal (TESTABLE WITHOUT API KEY)**
**What happens**: Query Neo4j for graph relationships around seed entities
**File**: `packages/tools/src/retrieval/HybridRetrievalTool.ts`
**Method**: `graphTraversal(seedEntities, userId, scenario, context)`
**Database**: Neo4j graph database
**Logic**: Uses CypherBuilder to construct safe traversal queries

**Test Commands**:
```bash
# Test: Create test graph data and verify traversal capability
echo "=== Testing Neo4j Graph Traversal ==="

# Create test graph nodes and relationships
docker exec neo4j-2d1l cypher-shell -u neo4j -p password123 "
CREATE (mu1:MemoryUnit {muid: 'mu_charles_columbia_research', user_id: 'dev-user-123', title: 'Columbia Research'})
CREATE (mu2:MemoryUnit {muid: 'mu_charles_health_issues', user_id: 'dev-user-123', title: 'Health Issues'})
CREATE (c1:Concept {concept_id: 'concept_autism_research', user_id: 'dev-user-123', name: 'Autism Research'})
CREATE (c2:Concept {concept_id: 'concept_work_life_balance', user_id: 'dev-user-123', name: 'Work Life Balance'})
CREATE (mu1)-[:RELATED_TO {relationship_label: 'influences', user_id: 'dev-user-123'}]->(c1)
CREATE (mu2)-[:RELATED_TO {relationship_label: 'exemplifies', user_id: 'dev-user-123'}]->(c2)
CREATE (c1)-[:RELATED_TO {relationship_label: 'impacts', user_id: 'dev-user-123'}]->(c2)
"

# Verify graph structure
echo "Graph structure created:"
docker exec neo4j-2d1l cypher-shell -u neo4j -p password123 "
MATCH (n) WHERE n.user_id = 'dev-user-123' 
RETURN labels(n) as type, n.muid as muid, n.concept_id as concept_id, n.name as name, n.title as title
"

# Test graph traversal query similar to HybridRetrievalTool
echo "Testing graph traversal from seed entity:"
docker exec neo4j-2d1l cypher-shell -u neo4j -p password123 "
MATCH (seed) WHERE (seed.muid = 'mu_charles_columbia_research' OR seed.concept_id = 'mu_charles_columbia_research') AND seed.user_id = 'dev-user-123'
MATCH (seed)-[r*1..2]-(neighbor) WHERE neighbor.user_id = 'dev-user-123'
RETURN DISTINCT neighbor, labels(neighbor) as type, r
"
```

**✅ Can test completely without API key**: Yes - direct Neo4j Cypher queries

**✅ EXPECTED RESULT**: Graph traversal returns related entities from seed nodes

### **6E: Stage 4 - PostgreSQL Pre-Hydration (TESTABLE WITHOUT API KEY)**
**What happens**: Fetch metadata for candidate entities from PostgreSQL
**File**: `packages/tools/src/retrieval/HybridRetrievalTool.ts`
**Method**: `preHydration(candidates, userId, context)`
**Database**: PostgreSQL memory_units and concepts tables
**Logic**: Batch fetch importance scores, creation dates, salience values

**Test Commands**:
```bash
# Test: Verify pre-hydration metadata fetching
echo "=== Testing PostgreSQL Pre-Hydration ==="

# Simulate candidate entities from previous stages
MEMORY_UNIT_IDS=("mu_charles_columbia_research" "mu_charles_health_issues" "mu_charles_family_situation")
CONCEPT_IDS=("concept_autism_research" "concept_work_life_balance")

# Test batch metadata fetch for memory units
echo "Memory unit metadata:"
docker exec postgres-2d1l psql -U danniwang -d twodots1line -c "
SELECT muid, importance_score, creation_ts, last_modified_ts 
FROM memory_units 
WHERE muid = ANY(ARRAY['mu_charles_columbia_research', 'mu_charles_health_issues', 'mu_charles_family_situation']) 
  AND user_id = 'dev-user-123';
"

# Test batch metadata fetch for concepts
echo "Concept metadata:"
docker exec postgres-2d1l psql -U danniwang -d twodots1line -c "
SELECT concept_id, salience, created_at, last_updated_ts 
FROM concepts 
WHERE concept_id = ANY(ARRAY['concept_autism_research', 'concept_work_life_balance']) 
  AND user_id = 'dev-user-123' AND status = 'active';
"
```

**✅ Can test completely without API key**: Yes - direct PostgreSQL queries

**✅ EXPECTED RESULT**: Metadata fetched successfully for all candidate entities

### **6F: Stage 5 - Entity Scoring & Prioritization (TESTABLE WITHOUT API KEY)**
**What happens**: EntityScorer calculates composite scores for entities
**File**: `packages/tools/src/retrieval/internal/EntityScorer.ts`
**Logic**: Weighted scoring (semantic similarity + recency + salience/importance)
**Formula**: `finalScore = α×semantic + β×recency + γ×salience`

**Test Commands**:
```bash
# Test: Verify scoring logic with manual calculations
echo "=== Testing Entity Scoring & Prioritization ==="

# Get current scoring weights configuration
cat config/retrieval_weights.json | jq '.scoring_weights.profiles.balanced' 2>/dev/null || echo "Default weights: semantic=0.5, recency=0.3, salience=0.2"

# Manual scoring test with sample data
node -e "
const now = Date.now();
const entities = [
  {id: 'mu_charles_columbia_research', type: 'MemoryUnit', weaviateScore: 0.85, importance_score: 8, created_at: new Date(now - 86400000)}, // 1 day ago
  {id: 'concept_autism_research', type: 'Concept', weaviateScore: 0.92, salience: 0.9, created_at: new Date(now - 172800000)}, // 2 days ago
  {id: 'mu_charles_health_issues', type: 'MemoryUnit', weaviateScore: 0.78, importance_score: 7, created_at: new Date(now - 259200000)} // 3 days ago
];

// Simulate scoring
entities.forEach(e => {
  const semantic = e.weaviateScore || 0.5;
  const recency = 1.0 - Math.min((now - e.created_at.getTime()) / (7 * 24 * 60 * 60 * 1000), 1.0); // 7-day decay
  const salience = e.importance_score ? e.importance_score / 10 : (e.salience || 0.5);
  const finalScore = 0.5 * semantic + 0.3 * recency + 0.2 * salience;
  console.log(\`\${e.id}: score=\${finalScore.toFixed(3)} (semantic=\${semantic.toFixed(3)}, recency=\${recency.toFixed(3)}, salience=\${salience.toFixed(3)})\`);
});
"
```

**✅ Can test completely without API key**: Yes - pure mathematical calculations

**✅ EXPECTED RESULT**: Entities scored and ranked by composite relevance

### **6G: Stage 6 - Full Content Hydration (TESTABLE WITHOUT API KEY)**
**What happens**: HydrationAdapter fetches full content for top-scored entities
**File**: `packages/tools/src/retrieval/internal/HydrationAdapter.ts`
**Method**: `hydrateTopEntities(scoredEntities, userId)`
**Database**: PostgreSQL full content fetch

**Test Commands**:
```bash
# Test: Verify full content hydration
echo "=== Testing Full Content Hydration ==="

# Simulate top entities from scoring (top 3)
TOP_ENTITIES=("mu_charles_family_situation" "concept_autism_research" "mu_charles_columbia_research")

# Test full memory unit hydration
echo "Full memory unit content:"
docker exec postgres-2d1l psql -U danniwang -d twodots1line -c "
SELECT muid, title, content, importance_score, creation_ts 
FROM memory_units 
WHERE muid = ANY(ARRAY['mu_charles_family_situation', 'mu_charles_columbia_research']) 
  AND user_id = 'dev-user-123'
ORDER BY importance_score DESC;
"

# Test full concept hydration
echo "Full concept content:"
docker exec postgres-2d1l psql -U danniwang -d twodots1line -c "
SELECT concept_id, name, type, description, salience, created_at 
FROM concepts 
WHERE concept_id = 'concept_autism_research' 
  AND user_id = 'dev-user-123' AND status = 'active';
"

# Test HydrationAdapter relationship enrichment (if Neo4j data exists)
echo "Relationship enrichment test:"
docker exec neo4j-2d1l cypher-shell -u neo4j -p password123 "
MATCH (source)-[r]->(target) 
WHERE (source.muid = 'mu_charles_family_situation' OR source.concept_id = 'concept_autism_research') 
  AND source.user_id = 'dev-user-123'
RETURN source.muid as source_muid, source.concept_id as source_concept, type(r) as relationship_type, target.muid as target_muid, target.concept_id as target_concept
LIMIT 10;
"
```

**✅ Can test completely without API key**: Yes - direct database content retrieval

**✅ EXPECTED RESULT**: Full entity content retrieved with relationships

---

## **STEP 7: Second LLM Call with Augmented Context (REQUIRES API KEY)**

### **7A: AugmentedMemoryContext Assembly**
**What happens**: HybridRetrievalTool returns structured context for second LLM call
**Expected Structure**:
```typescript
interface ExtendedAugmentedMemoryContext {
  relevant_memories: string[];
  contextual_insights: string[];
  retrievedMemoryUnits: MemoryUnit[];
  retrievedConcepts: Concept[];
  retrievalSummary: string;
  scoringDetails: ScoringDetails;
  performance_metadata: PerformanceMetadata;
}
```

**⚠️ Requires API key**: Second LLM call cannot proceed without working API

**🔴 EXPECTED RESULT**: Cannot test due to API geo-restriction

### **7B: Context-Aware Response Generation**
**What happens**: DialogueAgent makes second LLM call with memory context
**Expected**: More informed response using retrieved memories and concepts

**⚠️ Requires API key**: LLM processing required for context-aware generation

**🔴 EXPECTED RESULT**: Cannot test due to API geo-restriction

---

## **STEP 8: Redis Turn Context Storage (TESTABLE WITHOUT API KEY)**

### **8A: Turn Context Persistence**
**What happens**: DialogueAgent stores turn context in Redis
**File**: `services/dialogue-service/src/DialogueAgent.ts`
**Redis Key**: `turn_context:${conversationId}`
**TTL**: 600 seconds (10 minutes)
**Content**: `turn_context_package` from LLM response

**Test Commands**:
```bash
# Test: Redis turn context storage capability (manual simulation)
echo "=== Testing Redis Turn Context Storage ==="

# Simulate turn context data
TEST_CONTEXT='{"user_intent": "testing_hybrid_retrieval", "key_topics": ["research", "health", "career"], "emotional_state": "concerned_but_hopeful", "next_conversation_hints": ["explore specific university opportunities", "discuss traditional medicine options"]}'

# Test Redis storage
docker exec redis-2d1l redis-cli SET "turn_context:test-hybrid-retrieval-1" "$TEST_CONTEXT" EX 600

# Verify storage
echo "Stored turn context:"
docker exec redis-2d1l redis-cli GET "turn_context:test-hybrid-retrieval-1"

# Check TTL
echo "TTL remaining:"
docker exec redis-2d1l redis-cli TTL "turn_context:test-hybrid-retrieval-1"
```

**✅ Can test completely without API key**: Yes - direct Redis operations

**✅ EXPECTED RESULT**: Turn context stored and retrieved successfully

---

## **STEP 9: Response Assembly & Database Persistence (PARTIALLY TESTABLE)**

### **9A: Assistant Message Logging**
**What happens**: ConversationController logs assistant response to database
**Method**: `ConversationRepository.addMessage(conversationId, 'assistant', responseText, metadata)`
**Expected**: Assistant message persisted with execution metadata

**Test Commands**:
```bash
# Test: Simulate assistant message logging (without actual LLM response)
echo "=== Testing Assistant Message Persistence ==="

# Simulate assistant response
ASSISTANT_RESPONSE="Based on your situation, Charles, I understand the complexity of your decision. The intersection of your health concerns, research autonomy needs, and family considerations creates a compelling case for exploring the China opportunities. Let me help you think through the key factors systematically."

# Manual message logging test
docker exec postgres-2d1l psql -U danniwang -d twodots1line -c "
INSERT INTO conversation_messages (conversation_id, role, content, llm_call_metadata) 
VALUES ('test-hybrid-retrieval-1', 'assistant', '$ASSISTANT_RESPONSE', '{\"test_mode\": true, \"hybrid_retrieval_used\": false, \"execution_id\": \"da_test_123\"}');
"

# Verify logging
echo "Conversation messages:"
docker exec postgres-2d1l psql -U danniwang -d twodots1line -c "
SELECT conversation_id, role, substring(content, 1, 100) as content_preview, llm_call_metadata 
FROM conversation_messages 
WHERE conversation_id = 'test-hybrid-retrieval-1' 
ORDER BY timestamp DESC;
"
```

**✅ Can test without API key**: Yes - direct database operations

**✅ EXPECTED RESULT**: Assistant message logged successfully

---

## **COMPREHENSIVE END-TO-END HYBRID RETRIEVAL TEST (WITHOUT LLM)**

### **Manual Hybrid Retrieval Pipeline Test**
**Purpose**: Test complete 6-stage pipeline with manual inputs simulating LLM decision

```bash
echo "=== COMPREHENSIVE HYBRID RETRIEVAL PIPELINE TEST ==="

# Stage 1: Simulate key phrases from LLM decision
KEY_PHRASES=("Columbia research conflict" "autism protein targets" "traditional Chinese medicine" "green card concerns" "research autonomy")
echo "1. Key phrases: ${KEY_PHRASES[@]}"

# Stage 2: Test Weaviate semantic search for each phrase
echo "2. Weaviate semantic grounding:"
for phrase in "${KEY_PHRASES[@]}"; do
  echo "  Searching for: $phrase"
  curl -s -X POST "http://localhost:8080/v1/graphql" \
    -H "Content-Type: application/json" \
    -d "{\"query\": \"{ Get { UserKnowledgeItem(where: {path: [\\\"userId\\\"], operator: Equal, valueString: \\\"dev-user-123\\\"}, nearText: {concepts: [\\\"$phrase\\\"]}, limit: 2) { externalId sourceEntityType _additional { distance } } } }\"}" | jq -r '.data.Get.UserKnowledgeItem[]?.externalId // "no-results"' | head -2
done

# Stage 3: Test Neo4j graph traversal
echo "3. Neo4j graph traversal:"
SEED_ENTITIES=("mu_charles_columbia_research" "concept_autism_research")
for seed in "${SEED_ENTITIES[@]}"; do
  echo "  Traversing from: $seed"
  docker exec neo4j-2d1l cypher-shell -u neo4j -p password123 "
    MATCH (seed) WHERE (seed.muid = '$seed' OR seed.concept_id = '$seed') AND seed.user_id = 'dev-user-123'
    MATCH (seed)-[r*1..2]-(neighbor) WHERE neighbor.user_id = 'dev-user-123'
    RETURN DISTINCT neighbor.muid as muid, neighbor.concept_id as concept_id
    LIMIT 3
  " | tail -n +4 | head -3
done

# Stage 4: Test PostgreSQL metadata fetch
echo "4. PostgreSQL pre-hydration:"
ALL_CANDIDATES=("mu_charles_columbia_research" "mu_charles_health_issues" "concept_autism_research" "concept_work_life_balance")
docker exec postgres-2d1l psql -U danniwang -d twodots1line -c "
SELECT 'MemoryUnit' as type, muid as id, importance_score as score, creation_ts 
FROM memory_units 
WHERE muid = ANY(ARRAY['mu_charles_columbia_research', 'mu_charles_health_issues']) AND user_id = 'dev-user-123'
UNION ALL
SELECT 'Concept' as type, concept_id as id, salience as score, created_at as creation_ts 
FROM concepts 
WHERE concept_id = ANY(ARRAY['concept_autism_research', 'concept_work_life_balance']) AND user_id = 'dev-user-123'
ORDER BY score DESC;
"

# Stage 5: Simulate scoring (manual calculation shown earlier)
echo "5. Entity scoring complete (see previous scoring test)"

# Stage 6: Test content hydration for top entities
echo "6. Full content hydration for top 3 entities:"
docker exec postgres-2d1l psql -U danniwang -d twodots1line -c "
SELECT 'MemoryUnit' as type, muid as id, title, substring(content, 1, 100) as content_preview 
FROM memory_units 
WHERE muid = 'mu_charles_family_situation' AND user_id = 'dev-user-123'
UNION ALL
SELECT 'Concept' as type, concept_id as id, name as title, substring(description, 1, 100) as content_preview 
FROM concepts 
WHERE concept_id = 'concept_autism_research' AND user_id = 'dev-user-123';
"

echo "✅ Hybrid retrieval pipeline test complete!"
```

---

## **TESTING SUMMARY & NEXT STEPS**

### **✅ FULLY TESTABLE WITHOUT API KEY (Steps 1-4, 6, 8-9)**
1. **API Gateway Processing**: HTTP routing, authentication, conversation management
2. **Database Operations**: All PostgreSQL operations (users, conversations, messages, memory units, concepts)
3. **DialogueAgent Dependencies**: Dependency injection and initialization
4. **PromptBuilder Context**: All data fetching and template assembly
5. **Complete 6-Stage Hybrid Retrieval**: All database queries and processing logic
6. **Redis Operations**: Turn context storage and timeout key management
7. **Response Persistence**: Database logging and conversation state management

### **🔴 BLOCKED BY API KEY (Steps 5, 7)**
1. **LLM Processing**: Google Gemini API calls and response parsing
2. **Decision Routing**: LLM decision between `respond_directly` and `query_memory`
3. **Context-Aware Response**: Second LLM call with retrieved memory context

### **🎯 RECOMMENDED TESTING APPROACH**

1. **Execute Steps 1-4 and 6**: Test all non-LLM components comprehensively
2. **Focus on Hybrid Retrieval**: Thoroughly validate 6-stage pipeline with sample data
3. **Create Mock LLM Interface**: Build test doubles for LLM responses to enable end-to-end testing
4. **Performance Validation**: Measure database query performance across all stages

### **📋 SUCCESS CRITERIA**

**Infrastructure Tests**:
- ✅ All 4 databases operational without port conflicts
- ✅ Test data successfully created and queryable
- ✅ All dependencies inject correctly

**Hybrid Retrieval Tests**:
- ✅ 6-stage pipeline processes correctly with sample data
- ✅ Each stage produces expected intermediate results
- ✅ End-to-end retrieval returns relevant memory units and concepts
- ✅ Performance metrics within acceptable ranges

**Integration Tests**:
- ✅ DialogueAgent processes turn requests up to LLM boundary
- ✅ Database operations maintain consistency
- ✅ Redis operations work correctly
- 🔴 Complete end-to-end flow (pending API key resolution)

This testing plan provides comprehensive coverage of the DialogueAgent and hybrid retrieval system, with detailed focus on components that can be validated without external API dependencies. The granular test commands enable precise verification of each system component and integration point.
