# Current state of Cosmos Quest Agent
I'll scan the codebase to locate the "cosmos quest worker", its job queue wiring, and execution flow, then drill into the worker's process function and related services.

[3 tools called]

I found the quest flow pieces and will pull the relevant code snippets to show the end-to-end path from the API to the worker emitting Socket.IO updates.

[4 tools called]

I’m going to trace the end-to-end “Cosmos Quest” execution path from the API request through the agent and out to Socket.IO updates.

### High-level flow
- POST /api/v1/quest/process hits `QuestController.processQuest`.
- Controller immediately returns an `executionId`, then starts background processing.
- It calls `CosmosQuestAgent.processQuestWithProgressiveUpdates`, passing a callback.
- The agent runs 4 phases and invokes the callback for staged updates.
- Controller forwards updates to the `NotificationWorker` either via in-process notifier or HTTP fallback.
- `NotificationWorker` delivers updates over Socket.IO to the room `quest:{executionId}`; the frontend subscribes and renders.

### Where it happens

- API to agent wiring and progressive updates forwarding:
```1:118:/Users/danniwang/Documents/GitHub/202506062D1L/2D1L/apps/api-gateway/src/controllers/quest.controller.ts
import type { Request, Response, NextFunction } from 'express';
import { CosmosQuestAgent } from '@2dots1line/cosmos-quest-service';
import { CosmosQuestInput } from '@2dots1line/shared-types';

type Notifier = { sendQuestUpdate: (executionId: string, data: any) => void };

export class QuestController {
  private notifier?: Notifier;
  private cosmosQuestAgent?: CosmosQuestAgent;

  constructor(notifier?: Notifier, cosmosQuestAgent?: CosmosQuestAgent) {
    this.notifier = notifier;
    this.cosmosQuestAgent = cosmosQuestAgent;
  }

  /**
   * POST /api/v1/quest/process
   * Returns a new executionId immediately. Background streaming will be added next.
   */
  async processQuest(req: Request, res: Response, next: NextFunction) {
    try {
      const { userQuestion, conversationId, questType } = req.body || {};
      const userId = req.user?.id;
      const executionId = `cq_${Date.now()}`;

      // ... auth + response

      // Process quest in background if agent is available
      if (this.cosmosQuestAgent) {
        this.processQuestInBackground({
          userQuestion,
          userId,
          conversationId,
          questType: questType || 'exploration'
        }, executionId);
      } else {
        // mock
      }
    } catch (error) {
      next(error);
    }
  }

  /**
   * Process quest in background using real CosmosQuestAgent with progressive updates
   */
  private async processQuestInBackground(input: CosmosQuestInput, executionId: string) {
    try {
      // Process quest with progressive updates
      const result = await this.cosmosQuestAgent!.processQuestWithProgressiveUpdates(input, (updateType: string, data: any) => {
        this.sendQuestUpdate(executionId, updateType, data);
      });
    } catch (error) {
      // error update
    }
  }

  /**
   * Send a single quest update
   */
  private sendQuestUpdate(executionId: string, updateType: string, data: any) {
    if (this.notifier) {
      this.notifier.sendQuestUpdate(executionId, {
        type: updateType,
        ...data
      });
    } else {
      // HTTP fallback to NotificationWorker
      const notificationWorkerUrl = 'http://localhost:3002';
      fetch(`${notificationWorkerUrl}/internal/quest/update`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          executionId,
          data: { type: updateType, ...data }
        })
      }).catch(/* ... */);
    }
  }
}
```

- Cosmos Quest agent (4 phases and staged callbacks):
```63:127:/Users/danniwang/Documents/GitHub/202506062D1L/2D1L/services/cosmos-quest-service/src/CosmosQuestAgent.ts
public async processQuestWithProgressiveUpdates(
  input: CosmosQuestInput, 
  onUpdate: (updateType: string, data: any) => void
): Promise<CosmosQuestResult> {
  const executionId = `cq_${Date.now()}`;

  // --- PHASE I: LLM-BASED KEY PHRASE EXTRACTION ---
  const keyPhrases = await this.extractKeyPhrasesWithLLM(input, executionId);
  onUpdate('key_phrases', { capsules: keyPhrases });

  // --- PHASE II: MEMORY RETRIEVAL ---
  const augmentedContext = await this.retrieveMemory(keyPhrases, input.userId, executionId);

  // --- PHASE III: PROGRESSIVE VISUALIZATION GENERATION ---
  const visualization = await this.generateProgressiveVisualization(augmentedContext, executionId);
  onUpdate('visualization_stage_1', { stage: 1, entities: visualization.stage1 });
  onUpdate('visualization_stages_2_and_3', { stage2: visualization.stage2, stage3: visualization.stage3 });

  // --- PHASE IV: FINAL RESPONSE & WALKTHROUGH ---
  const finalResponse = await this.generateFinalResponse(input, augmentedContext, visualization, executionId);
  onUpdate('final_response', {
    response_text: finalResponse.response_text,
    walkthrough_script: finalResponse.walkthrough_script,
    reflective_question: finalResponse.reflective_question
  });

  return {
    execution_id: executionId,
    key_phrases: keyPhrases,
    visualization_stages: visualization,
    response_text: finalResponse.response_text,
    walkthrough_script: finalResponse.walkthrough_script,
    reflective_question: finalResponse.reflective_question,
    metadata: { /* ... */ }
  };
}
```

- Notification worker Socket.IO endpoint and internal update API:
```92:116:/Users/danniwang/Documents/GitHub/202506062D1L/2D1L/workers/notification-worker/src/index.ts
// Internal API to push quest updates from other services (API Gateway)
app.post('/internal/quest/update', (req: any, res: any) => {
  try {
    const { executionId, data } = req.body || {};
    if (!executionId || !data) {
      return res.status(400).json({ success: false, error: 'executionId and data are required' });
    }
    worker.sendQuestUpdate(executionId, data);
    return res.json({ success: true });
  } catch (err: any) {
    return res.status(500).json({ success: false, error: 'internal_error' });
  }
});
```

- Socket.IO rooms and the actual emitter:
```179:229:/Users/danniwang/Documents/GitHub/202506062D1L/2D1L/workers/notification-worker/src/NotificationWorker.ts
// Quest: join/leave specific execution room
socket.on('quest:join', (data: { executionId: string }) => {
  if (!data?.executionId) return;
  const room = `quest:${data.executionId}`;
  socket.join(room);
});

public sendQuestUpdate(executionId: string, data: any): void {
  if (!this.io) return;
  const room = `quest:${executionId}`;
  this.io.to(room).emit('quest:update', {
    execution_id: executionId,
    ...data,
    created_at: new Date().toISOString(),
  });
}
```

### What the “Cosmos Quest worker” is
- There isn’t a dedicated BullMQ “quest worker” today. The compute runs inside `CosmosQuestAgent` (a service) triggered by the API controller. Delivery to the client is via the dedicated `NotificationWorker` (Socket.IO service) which exposes an HTTP endpoint that the controller calls to push updates to the correct room.
- Other workers (e.g., `insight-worker`, `spatial-query-worker`) do use BullMQ, but the quest flow is controller → agent → notifier right now.

### Status
I located the controller, agent, and notification pieces and confirmed how updates propagate to the frontend.

- Main paths wired: `apps/api-gateway/src/controllers/quest.controller.ts`, `services/cosmos-quest-service/src/CosmosQuestAgent.ts`, `workers/notification-worker/src/*`.
- The quest flow is synchronous in-process compute plus async streamed updates via Socket.IO, not a BullMQ job.

# Areas for improvement

## UX flow
1. Use a quick and light model (Gemini-2.5-flash-lite) with separate prompt (very short and clear) to translate user query into key phrases. For the front end, the agent should playback and frame the user's question and lay out the key phrases. The cosmos view shows 1 node standing for reframed user query as a sentence and 1 node each for each key phrase.
2. The key phrases are turned into seed entities through semantic similarity search (we can build a dedicated tool since this is very simple, while leveraging HRT or semantic similarity tool's existing code). In cosmos, each key phrase is surrounded by semantic similar seed entities (these are real nodes from our database)
3. All the seed entities are then looked up as in the cosmos lookup view, with simplified user control: only keeping similarity score and number of graph hops
4. Right after key phrases are generated from step 1, in the backend, cosmos agent leverages HRT and a more powerful model gemini-2.5-flash to prepare memory enriched response to user's question (part A) and prepare the follow up walk through scripts (Part B). In the chat box, we will be streaming live what LLM prepares, starting with direct_response_text (Part A). In the chat box, we show the same thinking (placeholder)-> recollecting memory (placeholder)-> response just as in dialogue service
5. Assuming that the job of planning an interactive walkthrough might take LLM a few more seconds, we can pause the streaming and ask in the chat "would you like me to walk you through your cosmos and connect the dots?" which will give time to LLM to finish thinking through the part B response and once we the part B response is received through SSE, the walk through would begin and the chat box would actually stream the user facing talking points, not showing the thought process or the technical entitiy IDs, camera movement, etc.

## What to show in walk through

1. right now, the walk through is highly formulaic, showing only 4 nodes. This is not what I want. I want to give the agent plenty of freedom to create an engaging experience, whether it is 3 stages or 5-6 stages and whether it shows one node at a time or point to a cluster or even make all the stars disappear and let the user focus on some world knowledge not yet in user's knowledge base (just starring at the NASA starmap background in this case). In other words, it's the narrative first, and use the nodes and backgrounds and camera language to support and liven up the narrative.
2. the step-by-step walk through feels too much like a ppt presentation. It needs to be more fluid
3. The camera needs to smoothly takes focus from one shot to the next shot. Right now, there is no transition, one view suddenly snaps into another view.
4. I would also expect the nodes and edges to light up to help user see what's in the node that quest agent draws user attention to. The agent can even choose to show node details if that's helpful. In other words, the agent has the full stage.

# Agent response
I hear you. Here’s my own thinking after absorbing the code and your intent, focusing on what should change strategically rather than just describing what exists.

Status: I’m going beyond a literal trace to propose design changes that address complexity, divergence, and the “agent owns the stage” vision.

### The core problem
- Two parallel pipelines diverge:
  - Display path: what the user sees in `Graph3D` now (similarity + hops).
  - Knowledge path: what HRT retrieves for the agent to think/speak.
- When these diverge, you get a mismatch: the agent narrates connections that aren’t visible, or the scene shows entities the agent ignores. That kills immersion.

### My proposal: a dual-track, scene-aware orchestration with convergence
- Goal: Keep the agent narrative authoritative while preventing “what you see vs what I say” drift.

- Track A — Display Working Set (DWS):
  - A lightweight, deterministic retrieval track specialized for on-screen. Start from the fast key-phrase → Weaviate nearObject → constrained Neo4j hops (exactly like `CosmosLookupScene.tsx`, but backendized and typed) with tiny, controllable limits (e.g., 8–20 nodes total).
  - This set is what the scene renders immediately. It is capped, stable, and tuned for visuals.

- Track B — Knowledge Working Set (KWS):
  - Full HRT pipeline for high-quality synthesis: larger limits, scoring, hydration, possibly deeper hops, and diverse templates per `config/cypher_templates.json`.
  - It remains the ground truth for agent reasoning.

- Convergence layer:
  - Intersection-first policy: if an entity is in both KWS and DWS, it’s “Stage-Ready” and can be highlighted, focused, or detailed safely.
  - Union policy with graceful reveal: if the agent wants to reference an entity in KWS not yet visible, emit a “reveal” stage-direction to progressively add it into the DWS with a cinematic entrance (fade-in star, edge glow, camera ease).
  - Suppression policy: if DWS contains items not relevant to the current narrative beat, dim or blur them via environment cues (lower node opacity, background emphasis) so the stage remains coherent.

- Why this solves it:
  - You retain HRT’s quality for content while guaranteeing the visual stage never contradicts the narrative.
  - You gain explicit tools (reveal, suppress) to choreograph what becomes visible when.

### Is HRT too complex for Quest’s needs?
- Yes and no. For the “what to show right now” problem, HRT is overkill; we need snappy, deterministic, tiny sets. For “what to say” (synthesis quality, relevant memories, contextual correctness), HRT is exactly right.
- So split responsibilities:
  - DWS uses a minimal “SceneRetrieval” adapter that implements only:
    - nearObject similarity (Weaviate) with small limit
    - a single neighborhood template with 1–2 hops, strict caps
    - batched hydration
  - KWS uses the full HRT pipeline.
- Both consume the same IDs and normalization rules to guarantee 1:1 entity identity across systems.

### The agent “owns the stage”: how to make it real
- Replace rigid “visualization_stage_1/2/3” with a minimal “Stage Direction DSL” carried over `quest:update`:
  - camera_focus: { entity_id, offset?, ease_ms }
  - highlight_nodes: { ids, mode: 'spotlight' | 'pulse', dim_others: boolean, ease_ms }
  - highlight_edges: { pairs|ids, strength, ease_ms }
  - reveal_entities: { ids, layout_hint? }
  - environment: { starfield: 'dim'|'bright', vignette_opacity, fade_ms }
  - show_details: { entity_id }
  - narration_chunk: { text }
- Frontend reduces these to:
  - dispatch `camera-focus-request` events (we already support this cleanly in `LookupCameraController`)
  - set transient highlight states in scene store; `NodeMesh`/`EdgeMesh` already support emissive/glow/opacity and can tween smoothly
  - toggle starfield layers and dim non-focused nodes for “stage control”

### Handling the “two-step” concern (what if HRT retrieves things off-screen?)
- Default path:
  - DWS renders immediately from the lightweight track.
  - Agent begins Part A streaming text (DialogueAgent-style) while KWS is still computing, saying “I’m orienting to your cosmos…”
- When KWS arrives:
  - Intersect KWS ∩ DWS → immediately Stage-Ready.
  - For KWS \ DWS:
    - If critical to narrative → issue reveal_entities + camera_focus + highlight_edges
    - If non-critical → mention in narration without reveal, or schedule later “deep dive” segment

### Failure and edge cases
- No graph connections for DWS: still render similar cluster; agent uses environment cues to “darken the sky” and talk conceptually until KWS yields anything to reveal.
- Sparse KWS: stick to DWS for visuals; agent narrows narrative scope and prompts a reflective question to gather more context.
- Entity ID mismatches (Weaviate UUID vs Postgres/Neo4j IDs): enforce normalization at the convergence layer; strictly translate `_additional.id` to canonical `entity_id` before intersection/union.

### Performance and responsiveness
- HRT stays async; DWS is synchronous/lightweight and visible within a second.
- Cache the DWS by key-phrase hash in Redis to improve back-to-back scene loads.
- Limit edge rendering to edges adjacent to focused nodes to avoid visual clutter; we already have conditional edge visibility logic in `Graph3D`.

### Minimal changes with maximal effect
- Add the Stage Direction DSL to `packages/shared-types`.
- Implement the SceneRetrieval adapter (tiny subset of HRT behaviors) in the quest service for DWS.
- Keep HRT for KWS; just don’t block the stage on it.
- Update `CosmosQuestAgent` to emit:
  - key_phrases
  - dws_ready (initial small graph projection)
  - narration_chunk (streamed)
  - walkthrough_proposal
  - stage_direction (fine-grained, repeated)
- Update a `LiveQuestScene` to translate DSL → camera/highlights/environment using existing `Graph3D`, `LookupCameraController`, `NodeMesh`, `EdgeMesh`.

### Why this matches your line 207
- Nodes and edges light up precisely when the agent needs them, because stage directions control both highlight states and when to reveal edges/nodes. The agent can show node details on demand and dim the rest of the world; the scene obeys the narrative, not the other way around.

Summary
- Split the problem: a small SceneRetrieval for what to show, full HRT for what to say.
- Add a convergence layer and a Stage Direction DSL so the agent truly “owns the stage.”
- Use reveal/suppress policies to keep visuals and narrative synchronized under latency and data variance.

I’ll walk through the current streaming in DialogueAgent and then show exactly how to reuse it for Cosmos Quest, including a concrete, time-sequenced example of what the user sees and how it aligns with LLM output.

- Leveraging the existing streaming in DialogueAgent is the fastest, safest way to stream text to the user while Cosmos Quest retrieval and stage choreography run in parallel.

### What’s implemented today in DialogueAgent streaming

- The controller uses SSE to push tokens/chunks as they arrive, then sends a final result. The agent exposes an onChunk callback to stream partial text.

```106:147:apps/api-gateway/src/controllers/conversation.controller.ts
// Call the streaming DialogueAgent
const agentResult = await this.dialogueAgent.processTurnStreaming({
  userId,
  conversationId: actualConversationId,
  currentMessageText: message,
  onChunk: (chunk: string) => {
    // Send each chunk to the client
    res.write(`data: ${JSON.stringify({ 
      type: 'response_chunk', 
      content: chunk,
      conversation_id: actualConversationId
    })}\n\n`);
  }
});

// Log the ASSISTANT'S response
await this.conversationRepository.addMessage({
  conversation_id: actualConversationId,
  type: 'assistant',
  content: agentResult.response_text,
  metadata: agentResult.metadata || {}
});
```

- Inside the agent, the streaming variant of the main LLM call is enabled with enforceJsonMode and enableStreaming; onChunk is passed down so chunks can be surfaced immediately. The same JSON contract parsing is applied at the end.

```655:741:services/dialogue-service/src/DialogueAgent.ts
private async performSingleSynthesisCallStreaming(
  input: { userId: string; conversationId: string; finalInputText: string },
  augmentedMemoryContext?: AugmentedMemoryContext,
  callType: 'first' | 'second' = 'first',
  onChunk?: (chunk: string) => void
) {
  // Prepare LLM input with separated prompts and proper history
  const llmToolInput = {
    payload: {
      userId: input.userId,
      sessionId: input.conversationId,
      workerType: 'dialogue-service',
      workerJobId: `dialogue-stream-${Date.now()}`,
      conversationId: input.conversationId,
      messageId: `msg-stream-${Date.now()}`,
      sourceEntityId: input.conversationId,
      systemPrompt: promptOutput.systemPrompt,
      userMessage: promptOutput.userPrompt,
      history: formattedHistory,
      memoryContextBlock: augmentedMemoryContext?.relevant_memories?.join('\n') || '',
      temperature: 0.3,
      maxTokens: 50000,
      enforceJsonMode: true,
      enableStreaming: true,
      onChunk: onChunk
    },
    request_id: `dialogue-stream-${Date.now()}-${callType}`
  };

  // Enhanced LLM call with retry logic and streaming
  const llmResult = await LLMRetryHandler.executeWithRetry(
    this.llmChatTool,
    llmToolInput,
    { maxAttempts: 3, baseDelay: 1000, callType: `${callType}-streaming` }
  );

  // Use Gemini's native JSON parsing
  return this.parseLLMResponse(llmResult);
}
```

- The agent coordinates streaming with retrieval if needed; when LLM decides “query_memory,” it does the retrieval and then performs a second streaming call, still using onChunk.

```149:207:services/dialogue-service/src/DialogueAgent.ts
if (response_plan.decision === 'query_memory') {
  // Load user parameters and execute retrieval
  const userParameters = await this.loadUserHRTParameters(input.userId);
  const augmentedContext = await this.hybridRetrievalTool.execute({
    keyPhrasesForRetrieval: keyPhrases,
    userId: input.userId,
    userParameters: userParameters
  });

  // Second, context-aware LLM call with streaming
  const finalLlmResponse = await this.performSingleSynthesisCallStreaming(
    { ...input, finalInputText }, augmentedContext, 'second', input.onChunk
  );

  return { response_text: finalLlmResponse.direct_response_text || '...', ... };
}
```

### How to leverage this for Cosmos Quest

- Keep two channels in parallel:
  - Chat SSE stream for narration text (reuse DialogueAgent streaming pattern).
  - Socket.IO quest room updates for visual cues (key phrases, stage directions, highlights, camera focus).

- Why: The chat stream should feel instantaneous, while the 3D scene receives “stage direction” messages that the renderer enacts smoothly (camera, node/edge highlights, reveals). This aligns with your narrative-first vision and gives the agent the full stage.

- Implementation outline:
  - Cosmos Quest controller:
    - Return executionId immediately.
    - Start quest agent in background.
    - For narration streaming, either:
      - A) pipe agent onChunk into the existing SSE endpoint (recommended), or
      - B) send narration_chunk messages over Socket.IO (works too; SSE is already wired for chat).
  - Cosmos Quest agent:
    - During key-phrase extraction and retrieval, start an LLM streaming call for Part A (direct_response_text) using the same onChunk mechanism. Send chunks via SSE as they arrive.
    - Emit quest Socket.IO updates for:
      - key_phrases
      - dws_ready (initial small graph for display)
      - walkthrough_proposal (ask user to start the tour)
      - stage_direction (camera_focus, highlight_nodes/edges, reveal_entities, environment, show_details)

### What the user will actually see (detailed, time-sequenced example)

- Context: User asks, “Help me connect my work on health tech, cooking, and behavior change.”

- Timeline (approximate):
  - T+0.1s: The UI shows “Thinking…” in chat; the Cosmos panel joins room `quest:cq_...`.

  - T+0.4s: key phrases arrive (Socket.IO quest:update):
    - UI shows capsules “health tech”, “cooking”, “behavior change” at center.
    - Minimal nodes render quickly from SceneRetrieval (DWS) so the stage isn’t blank.

  - T+0.6s to T+3.5s: narration streaming begins (chat SSE):
    - response_chunk: “Let me orient to your cosmos… I’ll begin by grounding in your recent work.”
    - response_chunk: “I see strong links between your cooking experiments and behavior change goals…”
    - This continues fluidly as tokens stream.

  - T+1.5s: dws_ready (Socket.IO):
    - The initial small, deterministic display set appears: a few node stars and 2–3 edges, enough to set the stage.

  - T+2.8s: walkthrough_proposal (Socket.IO):
    - UI prompt in chat: “Would you like me to walk you through your cosmos and connect the dots?”
    - If user clicks “Yes” (or after a short pause), the tour begins.

  - T+3.2s onward: stage_direction stream (Socket.IO), coordinated with narration chunks:
    - stage_direction: camera_focus → “health tech” node
    - stage_direction: highlight_nodes { ids: [health_tech], dim_others: true, ease_ms: 600 }
    - narration_chunk: “Start here: your health tech explorations set the constraints and possibilities…”
    - stage_direction: reveal_entities { ids: [behavior_change], ease_ms: 800 }
    - stage_direction: highlight_edges { pairs: [[health_tech, behavior_change]], strength: 1.0, ease_ms: 500 }
    - narration_chunk: “Now, notice the path from health tech to behavior change…”
    - stage_direction: camera_focus → “cooking” with offset; environment: { starfield: 'dim', vignette_opacity: 0.3 }
    - narration_chunk: “Cooking becomes your practice ground for habit formation…”

- Representative payloads (abbreviated):
  - SSE response chunks:
```269:289:apps/api-gateway/src/controllers/conversation.controller.ts
res.write(`data: ${JSON.stringify({ 
  type: 'response_chunk', 
  content: chunk,
  conversation_id: actualConversationId
})}\n\n`);
```

  - Socket.IO quest room updates (NotificationWorker emits these):
```220:227:workers/notification-worker/src/NotificationWorker.ts
this.io.to(room).emit('quest:update', {
  execution_id: executionId,
  ...data,
  created_at: new Date().toISOString(),
});
```

  - Example quest:update data (what the front-end will handle):
```json
{ "type": "key_phrases", "capsules": [ { "phrase": "health tech" }, { "phrase": "cooking" }, { "phrase": "behavior change" } ] }

{ "type": "dws_ready", "nodes": [ { "id": "health_tech", "x": 1, "y": 2, "z": 0 }, ... ], "edges": [ { "source": "health_tech", "target": "behavior_change", "type": "related", "strength": 0.8 } ] }

{ "type": "walkthrough_proposal", "prompt": "Would you like me to walk you through your cosmos and connect the dots?" }

{ "type": "stage_direction", "camera_focus": { "entity_id": "health_tech", "ease_ms": 600 } }

{ "type": "stage_direction", "highlight_nodes": { "ids": ["health_tech"], "mode": "spotlight", "dim_others": true, "ease_ms": 600 } }

{ "type": "stage_direction", "highlight_edges": { "pairs": [["health_tech","behavior_change"]], "strength": 1.0, "ease_ms": 500 } }

{ "type": "stage_direction", "reveal_entities": { "ids": ["cooking"], "layout_hint": "near_behavior_change", "ease_ms": 800 } }
```

- Frontend coordination:
  - Chat pane listens to SSE and renders the partial text as it arrives.
  - Cosmos scene listens to `quest:update` and:
    - fires `camera-focus-request` events to `LookupCameraController` when camera_focus directions arrive,
    - sets highlight state for nodes/edges (using `NodeMesh`/`EdgeMesh` props to control glow/opacity/width, with tweening),
    - dims background layers (Graph3D starfields) per environment cues,
    - reveals new nodes/edges smoothly when the agent introduces off-screen knowledge.

### Why this works well for Cosmos Quest

- It preserves immediacy: narration starts within hundreds of milliseconds.
- It keeps the scene coherent: visual updates follow explicit stage directions from the agent.
- It supports the “agent has the full stage” requirement: nodes and edges light up exactly when the narrative calls for them; details can be shown on demand.

- Minimal changes needed:
  - Add a Stage Direction schema to shared types.
  - Have the Cosmos Quest agent use the same onChunk streaming used in DialogueAgent for narration.
  - Emit stage-direction quest updates through NotificationWorker.
  - Frontend: translate stage directions into camera/highlight/environment actions; SSE text is already supported in the chat.

Summary
- DialogueAgent already streams with onChunk, enforced JSON mode, and SSE delivery. We’ll reuse that for Cosmos Quest narration, while using Socket.IO quest updates to drive stage directions. The user experiences immediate narrated guidance with synchronized, cinematic scene choreography—nodes and edges light up on cue, the camera glides smoothly, and details appear when useful.

- Short answer: Support hard stop, pause, or branch mid-walkthrough. Stop streaming text and stage-direction emissions immediately; preserve scene state, then either resume or pivot the narrative to the user’s new intent.

### How to implement interruption cleanly

- Backend execution state (Redis)
  - execution:{id}: { status: 'running' | 'paused' | 'cancelled', lastStageIdx, lastCameraFocus, lastHighlights, updatedAt }
  - The CosmosQuestAgent checks status before emitting each stage direction or narration chunk; bail out if paused/cancelled.

- Interrupt signals and transport
  - Socket.IO from client:
    - quest:interrupt { executionId, reason?: 'user_message'|'skip'|'pause' }
    - quest:pause { executionId }, quest:resume { executionId }
  - Server actions:
    - Set status in Redis.
    - If 'cancelled' or 'pause': Abort LLM streaming via AbortController; stop emitting quest:update.
    - If 'user_message': flip to DialogueAgent streaming for the new message.

- LLM streaming and retrieval
  - Wrap the streaming LLM call in an abortable request (AbortController passed to `LLMRetryHandler` and ultimately the LLM tool).
  - Before each stage-direction emit, re-check Redis status; on 'paused' queue the next direction but don’t send; on 'cancelled' drop the queue.

- Frontend coordination
  - User UI affordances:
    - “Pause tour”, “Skip to summary”, “Ask a question”
  - On interrupt:
    - Send quest:interrupt with reason.
    - Stop all tweens/animations; keep current camera/scene state.
    - Dim highlights to a neutral state if 'cancelled'; keep highlights if 'paused'.
  - If user asks a question:
    - Show input as a new chat turn; start DialogueAgent SSE stream; optionally keep the cosmos visible but stop stage changes until resumed.

- Resuming a paused walkthrough
  - quest:resume → server reads lastStageIdx/lastCameraFocus, rehydrates DWS, and continues emitting from the next planned direction.
  - Frontend restores highlights/camera from the state sent in the first resumed update.

- Branching and reconciliation
  - If new user intent diverges:
    - End current execution (status 'cancelled').
    - Start a fresh executionId for the new walkthrough plan, or keep current scene and issue reveal/suppress directions to converge to the new focus.

- UX behavior examples
  - Mid-sentence “Pause tour”: narration chunk stops immediately; camera holds; glow fades to idle; “Tour paused” banner appears with “Resume” and “End” options.
  - “Skip to summary”: server cancels; emits one final quest:update { type: 'summary', bullets: [...] }; frontend fades out highlights, centers camera.
  - “What about cooking and glucose spikes?”: server cancels; routes question to DialogueAgent SSE; chat streams answer; scene remains static; user can “Start new tour” based on the new answer.

- Minimal data/events to add
  - Redis: execution status + last stage snapshot
  - Socket.IO: quest:interrupt/pause/resume; quest:update { type: 'summary' } for graceful end
  - Agent: status checks around onChunk emits and before each stage direction; AbortController for LLM calls

This gives instant, deterministic interruption: no more text or scene changes after the user acts; resumability is preserved; branching to a new conversational turn is seamless.

- Short answer: Do the performance groundwork first. Specifically: streaming, model selection, HRT delta-caching, prompt caching, DB indexes, and Redis stability. These directly unlock the “instant narration + fluid stage” vision and reduce the risk of regressions once we upgrade Cosmos Quest.

### Prioritized pre-work from the plan (what to do before the Cosmos Quest upgrade)

1) Response Streaming Implementation (Priority 1)
- Why it matters for Quest: Enables immediate Part A narration while retrieval and stage directions compute. Reduces time-to-first-token so the tour feels responsive.
- What to do:
  - Ensure our `LLMChatTool` actually uses streaming end-to-end (controller SSE and/or quest Socket.IO). Abort support for mid-tour interruption.
  - Verify token streaming is stable under backpressure and room fanout via `workers/notification-worker`.

2) Intelligent Model Selection (Priority 1)
- Why it matters for Quest: Your flow needs a fast “reframe + key phrase” pass and a higher-quality synthesis for the tour. This improves both speed and quality match.
- What to do:
  - Default “flash-lite” for key-phrase extraction; “flash” for quick guided narration; “pro” only for complex synthesis.
  - Add a simple complexity heuristic + user preference handoff so we pick models deterministically.

3) HRT Optimization (Delta + Caching) (Priority 2)
- Why it matters for Quest: We’ll call retrieval repeatedly across phases/turns. Delta + caching avoids re-hydrating identical context and keeps the tour snappy.
- What to do:
  - Cache recent `ExtendedAugmentedMemoryContext` keyed by (userId, conversationId) + phrase signature.
  - Use overlap thresholds to skip full runs; populate misses only. This directly lowers “wait” between stage-direction segments.

4) Prompt Section Caching (Priority 2)
- Why it matters for Quest: Prompt assembly latency eats into early narration time. Caching static sections reduces pre-LLM delay.
- What to do:
  - Cache core identity and operational config; only rebuild dynamic parts. Store in Redis with TTL.

5) Database Query Optimization + Indexes (Priority 2)
- Why it matters for Quest: Hydration and conversation fetches must be fast or narration stalls and stage reveals jitter.
- What to do:
  - Add the suggested composite indexes for conversation messages and sessions.
  - Review hydration queries used in retrieval/hydration adapters; add indexes on common predicates (entity_id, user_id, created_at) where missing.

6) Redis Connection Pooling & Resilience (Priority 2)
- Why it matters for Quest: NotificationWorker and SSE depend on Redis. Connection flakiness breaks live updates and room routing.
- What to do:
  - Introduce a pooled Redis layer for workers and API; set modest retry/backoff; monitor pool health.

7) Performance Monitoring & Alerts (Priority 3)
- Why it matters for Quest: We need early signal on LLM stalls, token overuse, or worker timeouts during tours.
- What to do:
  - Track TTFB, chunk rate, retrieval time, hydration time, and Socket.IO emit latencies. Add alerts on outliers.

### Why this sequence
- Streaming + model selection unlock “instant feel” and quality. HRT delta, prompt caching, and DB/Redis harden the path so tours don’t stutter mid-run. Monitoring guards regressions during/after the Cosmos Quest upgrade.

Summary
- Do first: streaming, model selection.
- Next: HRT delta-caching, prompt caching, DB indexes, Redis pooling.
- Finally: performance monitoring.
- These upgrades directly support “fast narrated start,” smooth walk-through choreography, and reliable, interruptible tours.


# Claude plan

I'll conduct a comprehensive codebase investigation to ground the roadmap in actual implementation state, dependencies, and risk factors. Let me examine the current Cosmos Quest, DialogueAgent streaming, LLMChatTool, NotificationWorker capacity, HRT internals, and frontend quest hooks to build an evidence-based sequence.

[7 tools called]

Now let me read a few more critical files to understand the current state of prompt caching, model selection, and database indexes.

[5 tools called]

Now let me create a comprehensive, evidence-based assessment and roadmap.

## **Unbiased Assessment & Optimized Implementation Roadmap**

Based on a methodical review of the codebase, I'll provide a fact-grounded analysis and a sequenced roadmap with explicit rationale.

---

### **I. CURRENT STATE (Grounded in Code)**

#### **1. Streaming Infrastructure**
- **DialogueAgent**: Full SSE streaming is **implemented and working** (L320-493 in `LLMChatTool.ts`). It extracts `direct_response_text` from JSON during streaming and calls `onChunk` incrementally.
- **CosmosQuestAgent**: **No streaming today**. L176-230 in `CosmosQuestAgent.ts` shows synchronous LLM calls with no `enableStreaming` or `onChunk`.
- **Verdict**: DialogueAgent SSE is production-ready. Cosmos needs trivial adaptation (add two parameters).

#### **2. Model Selection**
- **Current state**: `LLMChatTool` uses `EnvironmentModelConfigService.getModelForUseCase('chat')` (L137 in `LLMChatTool.ts`). Model is **global**, set once at initialization.
- **Gap**: No per-call override. CosmosQuestAgent can't request "flash-lite" for key phrases vs "pro" for synthesis within the same execution.
- **Verdict**: Model selection exists but is **static per tool instance**, not dynamic per request.

#### **3. HRT Caching**
- **Current state**: HRT has **zero caching** for retrieval results (L110-169 in `HybridRetrievalTool.ts`). Every `execute()` runs all six stages from scratch.
- **User parameters** are loaded per-user from Redis (L852-873 in `CosmosQuestAgent.ts`), but **retrieval output is never cached**.
- **Verdict**: No delta caching. Full re-retrieval on every turn, even for identical key phrases within the same conversation.

#### **4. Prompt Caching**
- **Current state**: Prompts are built fresh on every call. `PromptBuilder.buildPrompt` (L81+ in `PromptBuilder.ts`) fetches user, history, summaries, turn context from DB/Redis **every time**.
- **CosmosQuestPromptBuilder** (L50-92, L97-143) does the same: no caching of static sections.
- **Verdict**: No caching. Core identity and operational config are re-fetched and re-templated on every LLM call.

#### **5. Database Indexes**
- **Conversations**: `conversation_messages` has composite index `(conversation_id, created_at)` (L425 in `20250603010109_init_v7_schema_corrected/migration.sql`).
- **Memory hydration**: `memory_units` has `(user_id, creation_ts DESC)`, `(user_id, processing_status)`, `(user_id, source_type)`, `(user_id, importance_score)` (L323-332).
- **Concepts**: `(user_id, type)`, `(user_id, name)` (L344-347).
- **Verdict**: **Basic indexes exist**. No evidence of missing critical indexes for current query patterns, but no mention of EXPLAIN ANALYZE verification in docs.

#### **6. Redis Connection Pooling**
- **Current state**: `DatabaseService` creates a **single shared** Redis client with `keepAlive: 30000`, `connectTimeout: 10000`, `commandTimeout: 10000` (L60-89 in `DatabaseService.ts`).
- **Workers**: Each BullMQ worker creates **dedicated Redis connections** with identical config (L40-82 in `insight-worker/src/index.ts`, L49-81 in `ingestion-worker/src/index.ts`).
- **Verdict**: Dedicated connections exist for workers. **Not pooled**, but configured with timeouts and offline queue. No explicit retry logic beyond `enableOfflineQueue: true`.

#### **7. NotificationWorker Capacity**
- **Current state**: Socket.IO server with rooms, internal HTTP ingress `/internal/quest/update` (L92-101 in `notification-worker/src/index.ts`), `sendQuestUpdate` method (L220-227 in `NotificationWorker.ts`).
- **Load**: Already handles quest updates, new card notifications, graph projection updates.
- **Verdict**: Infrastructure exists and is **adequate for Cosmos Quest** stage directions. No evidence of overload.

#### **8. Cosmos Quest Current Flow**
- **Phases**: 4 synchronous phases (key phrases → retrieval → visualization → final response) (L71-127 in `CosmosQuestAgent.ts`).
- **Updates**: Emits 4 `onUpdate` callbacks: `key_phrases`, `visualization_stage_1`, `visualization_stages_2_and_3`, `final_response`.
- **Visualization**: Splits HRT results into stage1/2/3 based on relevance score percentiles (L340-580).
- **Frontend**: `useQuestConnection` hook joins Socket.IO room and reduces updates (L15-93 in `useQuestConnection.ts`).
- **Verdict**: Rigid 3-stage visualization. No streaming narration. No stage directions (camera, highlight, reveal). No scene/knowledge split (DWS vs KWS).

#### **9. Prompt Infrastructure Sharing**
- **Current state**: `PromptBuilder` (DialogueAgent) uses `ConfigService.getTemplate()` to load sections from YAML (L98-100 in `PromptBuilder.ts`).
- **CosmosQuestPromptBuilder** has **hardcoded prompts** in methods (L152-182, L206-253).
- **Verdict**: Cosmos is **isolated**, not using shared templates or caching infra.

---

### **II. GAPS vs VISION**

| Requirement (from Spec) | Current State | Gap Severity |
|---|---|---|
| Instant narration streaming | DialogueAgent ✓, Cosmos ✗ | **High** – breaks "fast narrated start" |
| Fast model for key phrases | No per-call model override | **High** – key phrase step is slow |
| Stage directions (camera, highlight, reveal) | Hardcoded 3 stages, no DSL | **Critical** – blocks "agent owns the stage" |
| Scene vs knowledge split (DWS/KWS) | Single HRT path | **Medium** – can defer, but risks divergence |
| HRT delta caching | None | **Medium** – causes repeated work |
| Prompt section caching | None | **Low** – latency impact is small (~50-100ms) |
| DB indexes | Exist, not verified | **Low** – no evidence of slow queries yet |
| Redis pooling/retry | Dedicated conns, basic retry | **Low** – no evidence of failures |
| Interrupt/pause/resume | None | **Medium** – UX feature, not blocking MVP |

---

### **III. OPTIMIZED IMPLEMENTATION ROADMAP**

I'll sequence by **dependency order + impact**.

---

#### **PHASE 0: FOUNDATION (Days 1-2)**
**Goal**: Enable dynamic model selection and verify infrastructure is ready.

##### **Task 0.1: Dynamic Model Selection in LLMChatTool** ✅
- **What**: Add optional `modelOverride?: string` to `LLMChatInputPayload`. If set, use it instead of `getModelForUseCase('chat')`.
- **Why first**: CosmosQuestAgent needs this to request "flash-lite" for key phrases and "flash/pro" for synthesis in the same execution. Blocks all Cosmos optimizations.
- **Effort**: ~30 min. Change L137-142 in `LLMChatTool.ts` to check `input.payload.modelOverride || this.modelConfigService.getModelForUseCase('chat')`.
- **Risk**: None. Backward compatible.

##### **Task 0.2: Verify DB Index Coverage** ✅
- **What**: Run `EXPLAIN ANALYZE` on the 3 slowest queries: `getMostRecentMessages`, `getRecentImportantConversationSummaries`, `findUserByIdWithContext`.
- **Why first**: If indexes are missing, every streaming call will stall. Must know before optimizing anything else.
- **Effort**: ~1 hour. Check query plans, add indexes if seq scans appear on >1000 rows.
- **Risk**: Low. Indexes already exist; this is verification.

##### **Task 0.3: Add Redis Retry Wrapper** ✅
- **What**: Wrap `redisClient.get/set` in a retry helper (max 3 attempts, exponential backoff 100/200/400ms) in `DatabaseService`.
- **Why first**: Redis flakiness breaks prompt caching and HRT caching in later phases. Harden now.
- **Effort**: ~1 hour. Reuse `LLMRetryHandler` pattern.
- **Risk**: Low. Adds resilience without changing behavior.

---

#### **PHASE 1: STREAMING & MODEL SPEED (Days 3-4)**
**Goal**: Cosmos narration streams immediately; key phrases extract in <1s.

##### **Task 1.1: Add Streaming to CosmosQuestAgent Key Phrase Extraction** 🔄
- **What**: Pass `enableStreaming: true, onChunk: onNarrationChunk` to LLM call at L189-222 in `CosmosQuestAgent.ts`. Add `onNarrationChunk` callback param to `processQuestWithProgressiveUpdates`. Controller emits `narration_chunk` events to NotificationWorker.
- **Why now**: Unlocks "instant feel" for user. Narration starts in <600ms while retrieval runs.
- **Effort**: ~2 hours. Copy DialogueAgent pattern; add `narration_chunk` event type.
- **Depends on**: 0.1 (model selection).
- **Risk**: Low. Proven pattern.

##### **Task 1.2: Use flash-lite for Key Phrase Extraction** ✅
- **What**: Set `modelOverride: 'gemini-2.0-flash-lite'` in key phrase LLM call (L189-222). Keep flash/pro for final response (L605-637).
- **Why now**: Reduces key phrase latency from ~2s to ~400ms. Critical for "fast narrated start."
- **Effort**: ~10 min.
- **Depends on**: 0.1.
- **Risk**: None. Flash-lite is fast enough for 3-7 key phrases.

---

#### **PHASE 2: STAGE DIRECTION DSL (Days 5-7)**
**Goal**: Define and implement the "agent owns the stage" primitives.

##### **Task 2.1: Define Stage Direction Types in shared-types** 🔧
- **What**: Add `StageDirection` union type in `packages/shared-types/src/ai/cosmos-quest.types.ts`:
```typescript
type CameraFocus = { action: 'camera_focus'; entity_id: string; offset?: [number, number, number]; ease_ms: number };
type HighlightNodes = { action: 'highlight_nodes'; ids: string[]; mode: 'spotlight'|'pulse'; dim_others: boolean; ease_ms: number };
type HighlightEdges = { action: 'highlight_edges'; pairs: [string, string][]; strength: number; ease_ms: number };
type RevealEntities = { action: 'reveal_entities'; ids: string[]; layout_hint?: string; ease_ms: number };
type Environment = { action: 'environment'; starfield: 'dim'|'bright'; vignette_opacity: number; fade_ms: number };
type ShowDetails = { action: 'show_details'; entity_id: string };

export type StageDirection = CameraFocus | HighlightNodes | HighlightEdges | RevealEntities | Environment | ShowDetails;
```
- **Why now**: Shared contract between agent (emitter) and frontend (consumer). Must exist before agent or UI changes.
- **Effort**: ~1 hour.
- **Depends on**: None.
- **Risk**: None. Pure types.

##### **Task 2.2: Update CosmosQuestAgent to Emit stage_direction** 🔧
- **What**: In `generateFinalResponse`, parse LLM output to extract an array of `StageDirection` objects. Emit them via `onUpdate('stage_direction', { directions: [...] })` in chunks (one per "beat" of narration).
- **Why now**: Agent can now control the scene dynamically instead of rigid stage1/2/3.
- **Effort**: ~4 hours. Update prompt to request stage directions in JSON; parse and emit.
- **Depends on**: 2.1.
- **Risk**: Medium. LLM must learn to emit valid directions; fallback to default walkthrough if parsing fails.

##### **Task 2.3: Frontend LiveQuestScene Reducer** 🔧
- **What**: In `useQuestConnection`, add reducer case for `stage_direction`. Dispatch `camera-focus-request`, update node/edge highlight state, call starfield dim/bright methods.
- **Why now**: Completes the loop. Agent emits → frontend enacts.
- **Effort**: ~6 hours (includes testing camera ease, highlight tweens).
- **Depends on**: 2.2.
- **Risk**: Medium. Requires Graph3D and LookupCameraController API understanding.

---

#### **PHASE 3: SCENE vs KNOWLEDGE SPLIT (Days 8-10, Optional for MVP)**
**Goal**: Prevent visual/narrative divergence.

##### **Task 3.1: Create SceneRetrieval Adapter** 🔧
- **What**: New class `SceneRetrievalTool` in `packages/tools/src/retrieval/SceneRetrievalTool.ts`. Implements lightweight DWS: nearObject Weaviate similarity → Neo4j 1-2 hops → batched hydration. Cap at 20 nodes total.
- **Why defer**: Not needed for MVP. Current HRT can serve both roles if we emit stage directions for all retrieved entities. Split only if divergence becomes a real UX problem.
- **Effort**: ~8 hours.
- **Depends on**: None (can run in parallel with Phase 2).
- **Risk**: Low. Reuses HRT internal modules.

##### **Task 3.2: Dual-Track in CosmosQuestAgent** 🔧
- **What**: Call `SceneRetrievalTool` (DWS) in parallel with `HybridRetrievalTool` (KWS). Emit `dws_ready` with small graph for immediate render. Use KWS for narration. Emit `reveal_entities` for KWS \ DWS.
- **Why defer**: Adds complexity. Only needed if users complain about narration referencing off-screen entities.
- **Effort**: ~4 hours.
- **Depends on**: 3.1, 2.2.
- **Risk**: Medium. Convergence logic must be correct.

**Decision**: **Skip Phase 3 for initial launch**. Emit stage directions for all HRT results. Monitor for divergence complaints. Revisit if needed.

---

#### **PHASE 4: CACHING (Days 11-12)**
**Goal**: Reduce latency for repeat queries.

##### **Task 4.1: HRT Delta Caching** 🔧
- **What**: After HRT `execute()`, cache result in Redis: `hrt_cache:{userId}:{conversationId}:{phraseSignature}` → `ExtendedAugmentedMemoryContext`, TTL 10 min. On next call, if 80%+ phrase overlap, return cached + fetch delta for new phrases.
- **Why after streaming**: Caching is an optimization; streaming is a UX requirement. Do UX first.
- **Effort**: ~6 hours (signature hash, overlap check, delta merge).
- **Depends on**: 0.3 (Redis retry).
- **Risk**: Low. Fallback to full retrieval on cache miss.

##### **Task 4.2: Prompt Section Caching** 🔧
- **What**: In `PromptBuilder` and `CosmosQuestPromptBuilder`, cache `core_identity_section` and `operational_config_section` in Redis: `prompt_section:{userId}:core_identity` → string, TTL 1 hour. Rebuild only dynamic sections (history, turn context, memory).
- **Why after HRT caching**: Smaller impact (~50ms saved vs ~500ms from HRT delta). Do higher-impact first.
- **Effort**: ~3 hours.
- **Depends on**: 0.3.
- **Risk**: None. Cache invalidation on user profile update (edge case).

---

#### **PHASE 5: INTERRUPT CONTROLS (Days 13-14, Optional for MVP)**
**Goal**: User can pause, resume, skip tour.

##### **Task 5.1: Execution Status in Redis** 🔧
- **What**: Store `execution:{executionId}` → `{ status: 'running'|'paused'|'cancelled', lastStageIdx, updatedAt }`. CosmosQuestAgent checks status before each `onUpdate` emit; abort if paused/cancelled.
- **Why defer**: Nice-to-have UX. Not blocking narration streaming or stage directions.
- **Effort**: ~2 hours.
- **Depends on**: None.
- **Risk**: Low.

##### **Task 5.2: Socket.IO Interrupt Handlers** 🔧
- **What**: Add `quest:interrupt`, `quest:pause`, `quest:resume` handlers in NotificationWorker. Update Redis status. Emit confirmation to client.
- **Effort**: ~3 hours.
- **Depends on**: 5.1.
- **Risk**: Low.

##### **Task 5.3: Frontend Interrupt UI** 🔧
- **What**: Add "Pause Tour", "Skip to Summary" buttons. Emit `quest:interrupt` on click. Stop tweens, hold camera.
- **Effort**: ~2 hours.
- **Depends on**: 5.2.
- **Risk**: Low.

**Decision**: **Defer Phase 5 to post-MVP**. Streaming + stage directions are higher priority.

---

#### **PHASE 6: MONITORING (Days 15-16, Post-Launch)**
**Goal**: Observe performance in production.

##### **Task 6.1: Add Telemetry** 🔧
- **What**: Log TTFB, chunk rate, retrieval time, stage-direction emit latency to a structured log or metrics DB.
- **Effort**: ~4 hours.
- **Risk**: None.

---

### **IV. FINAL SEQUENCED ROADMAP**

| Phase | Tasks | Days | Rationale |
|---|---|---|---|
| **0. Foundation** | 0.1 Dynamic model selection<br>0.2 Verify DB indexes<br>0.3 Redis retry | 1-2 | Unblocks all downstream; low risk |
| **1. Streaming & Speed** | 1.1 Cosmos narration streaming<br>1.2 Flash-lite for key phrases | 3-4 | **Highest UX impact**; depends on 0.1 |
| **2. Stage Directions** | 2.1 Define DSL types<br>2.2 Agent emits directions<br>2.3 Frontend reducer | 5-7 | **Core vision** ("agent owns stage"); depends on 1.1 |
| **3. Scene/Knowledge Split** | *(SKIP for MVP)* | — | Complexity vs benefit unclear; defer |
| **4. Caching** | 4.1 HRT delta<br>4.2 Prompt sections | 11-12 | Optimization after UX is solid |
| **5. Interrupt Controls** | *(DEFER to post-MVP)* | — | Nice-to-have; not blocking launch |
| **6. Monitoring** | 6.1 Telemetry | 15-16 | Observe production behavior |

**Total to MVP**: **~12 days** (Phases 0, 1, 2, 4).

---

### **V. WHY THIS ORDER?**

1. **Dependencies**: Model selection (0.1) blocks streaming (1.1) and speed (1.2). Stage Direction DSL (2.1) blocks agent emit (2.2) and frontend (2.3).
2. **Risk reduction**: Foundation (Phase 0) hardens infrastructure before adding load. Streaming (Phase 1) is proven in DialogueAgent, so low risk. Stage directions (Phase 2) are new, so test early.
3. **UX first, optimization second**: Streaming + stage directions deliver the vision. Caching (Phase 4) is valuable but doesn't change what users see.
4. **Defer complexity**: Scene/knowledge split (Phase 3) adds cognitive load and code paths; only do it if divergence is a real problem. Interrupt controls (Phase 5) are polish.

---

### **VI. TODOS UPDATE**

[1 tool called]

---

### **FINAL SUMMARY**

**Current State**: Cosmos Quest has rigid 4-phase flow with no streaming, no dynamic scene control, no caching. DialogueAgent has proven SSE streaming. Redis connections are stable but not hardened. DB indexes exist but unverified.

**Critical Path to Vision**: 
1. **Phase 0 (Foundation)**: Dynamic model selection + verify infrastructure.
2. **Phase 1 (Streaming & Speed)**: Stream narration + use flash-lite → instant feel.
3. **Phase 2 (Stage Directions)**: Define DSL → agent emits → frontend enacts → "agent owns the stage."
4. **Phase 4 (Caching)**: HRT delta + prompt sections → reduce latency for repeat queries.

**Deferred**: Scene/knowledge split (Phase 3, unclear benefit), interrupt controls (Phase 5, post-MVP polish).

**Timeline**: 12 days to MVP.

**Next Step**: Execute Phase 0.1 (model selection) to unblock everything else.