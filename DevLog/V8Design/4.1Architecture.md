Understood. The objective is to create a definitive, standalone, and exhaustive canonical guide for the V8.5 "Inter-Agent Message Board" architecture. This document will be constructed from first principles, assuming no prior context, and will serve as the absolute source of truth for the system's cognitive flow, prompt engineering, and agent responsibilities.

I will be meticulous in detailing every component, data flow, and dependency to withstand rigorous interrogation.

---

### **Canonical Guide: The V8.5 Multi-Layered Prompt & State Propagation System**

#### **1. Core Philosophy: The "Inter-Agent Message Board"**

The 2dots1line system's intelligence emerges from a sophisticated, multi-layered communication system between three specialized AI agents. This system is architecturally analogous to a shared message board with "sticky notes" left by agents for each other and for their future selves. This approach solves the "forgetful LLM" problem by creating a persistent, evolving state that informs every interaction at three distinct timescales: the immediate turn, the subsequent conversation, and the long-term strategic cycle.

Every major agent operation concludes by generating a structured **`Forward-Looking Context Package`**, which is the "sticky note" it posts to the message board (our PostgreSQL database) for the next agent in the chain to read.

The three agents and their temporal domains are:

1.  **`DialogueAgent`**: The **Real-Time Agent**. It operates on the timescale of a single conversational turn. Its primary concern is maintaining immediate coherence and responsiveness.
2.  **`IngestionAnalyst`**: The **Post-Conversation Agent**. It operates on the timescale of a completed conversation. Its primary concern is integrating the "just now" into the user's permanent knowledge graph and preparing for the *next* conversation.
3.  **`InsightEngine`**: The **Cyclical Agent**. It operates on a long-term, periodic cycle (e.g., weekly). Its primary concern is strategic synthesis, ontology refinement, and long-term growth planning.

---

#### **2. The "Sticky Notes": Persistent State Structures in the Database**

Our "message board" is implemented using specific JSONB fields in the PostgreSQL database. These are the persistent data structures that enable our state propagation.

**File Location:** `packages/database/prisma/schema.prisma`

| Model & Field                                     | "Sticky Note" Analogy                                | Written By           | Read By                                    | Purpose                                                                                                                              |
| :------------------------------------------------ | :--------------------------------------------------- | :------------------- | :----------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------- |
| **`Conversation.turn_context_package`**           | A note passed between two people during a meeting.   | `DialogueAgent`      | `DialogueAgent` (on the next turn)         | Maintains tactical, turn-by-turn coherence and adaptive state *within* a single, active conversation.                               |
| **`User.next_conversation_context_package`**      | A debrief memo written after a meeting to prepare for the next one. | `IngestionAnalyst`   | `DialogueAgent` (at the start of a new conversation) | Bridges the gap between conversations, providing proactive greetings and follow-ups based on the *last* completed conversation. |
| **`User.memory_profile`**                         | A comprehensive annual strategic review document.    | `InsightEngine`      | *All Agents* (primarily `DialogueAgent` and `IngestionAnalyst`) | Provides a deep, long-term, synthesized understanding of the user's core identity, goals, and knowledge structure. It's the foundational context. |

---

#### **3. The `PromptBuilder`: Assembling the Complete Context**

The `PromptBuilder` is a deterministic service that acts as the "board reader." At the start of every `DialogueAgent` turn, it reads all the relevant "sticky notes" and assembles them into a single, perfectly structured prompt for the LLM.

**File Location:** `services/cognitive-hub/src/services/PromptBuilder.ts`

##### **`buildPrompt` Method Workflow:**

1.  **Input:** Receives `{ userId, conversationId }`.
2.  **Parallel Data Fetch:** It simultaneously fetches:
    *   The `CoreIdentity.yaml` content (from Redis cache).
    *   The entire `User` record, which contains `memory_profile` and `next_conversation_context_package`.
    *   The current `Conversation` record, which contains `turn_context_package`.
    *   The recent message history for the `conversationId`.
3.  **Assembly:** It formats each piece of data into a clearly demarcated XML-tagged block.
4.  **Output:** Returns a single, massive string containing all context blocks, which becomes the system prompt for the `DialogueAgent`'s LLM call.

##### **Structure of the Assembled Prompt:**

```xml
<system_identity>
  <!-- Content from CoreIdentity.yaml -->
</system_identity>

<user_memory_profile>
  <!-- Content from User.memory_profile (written by InsightEngine) -->
</user_memory_profile>

<context_from_last_conversation>
  <!-- Content from User.next_conversation_context_package (written by IngestionAnalyst) -->
  <!-- This block is ONLY included for the FIRST turn of a new conversation. -->
</context_from_last_conversation>

<context_from_last_turn>
  <!-- Content from Conversation.turn_context_package (written by DialogueAgent) -->
  <!-- This block is included for all turns EXCEPT the first. -->
</context_from_last_turn>

<current_conversation_history>
  <!-- A transcript of the last N messages in this conversation. -->
</current_conversation_history>

<instructions>
  <!-- The task-specific instructions for the LLM for this turn. -->
</instructions>
```

---

#### **4. Detailed Agent Workflows & "Sticky Note" I/O**

This section details precisely what each agent consumes as input and produces as output.

##### **A. `DialogueAgent`: The Real-Time Agent**

*   **INPUT PROMPT COMPONENTS:**
    1.  `CoreIdentity`: To know its own persona and rules.
    2.  `userMemoryProfile`: To have deep, long-term context about the user.
    3.  `next_conversation_context_package` (**First Turn Only**): To start the conversation proactively.
    4.  `turn_context_package` (**All Subsequent Turns**): To remember what its focus was from the previous turn.
    5.  `current_conversation_history`: To have immediate conversational context.

*   **WORKFLOW (The "Single Synthesis Call"):**
    1.  The agent receives the user's message.
    2.  It calls the `PromptBuilder` to assemble the complete prompt as described above.
    3.  It makes a **single call** to the `LLMChatTool`. The instructions ask the LLM to perform its "Router" logic (decide whether to `respond_directly` or `query_memory`) and generate a complete JSON response.

*   **OUTPUT "STICKY NOTE" (The `turn_context_package`):**
    *   **Generated By:** The `DialogueAgent`'s LLM call.
    *   **Stored In:** `Conversation.turn_context_package` (overwriting the previous turn's note).
    *   **JSON Structure & Purpose:**
        ```json
        {
          "summary_of_last_turn": "A concise summary of the user's last message and the agent's response. This prevents topic drift.",
          "suggested_next_focus": "A specific focus for the next turn, e.g., 'Ask a clarifying question about their stated fear of failure.'",
          "emotional_tone_to_adopt": "The emotional register to use, e.g., 'empathetic', 'curious', 'action-oriented'. This makes the agent feel more adaptive.",
          "memory_retrieval_needed": {
            "status": "false" // This would be true if the router decided to query memory.
          },
          "flags_for_ingestion": ["potential_new_goal_detected", "high_emotional_intensity"] // Flags for the future IngestionAnalyst.
        }
        ```

##### **B. `IngestionAnalyst`: The Post-Conversation Agent**

*   **INPUT PROMPT COMPONENTS:**
    1.  `CoreIdentity` (subset): Needs to know its role as an analyst.
    2.  `userMemoryProfile`: To avoid creating duplicate concepts and to understand the new information in context.
    3.  **Full Conversation Transcript:** Its primary data source.
    4.  All `turn_context_package` flags accumulated during the conversation.

*   **WORKFLOW (The "Single Synthesis Call"):**
    1.  The worker is triggered by a completed `conversationId`.
    2.  It gathers all the required input components.
    3.  It makes a **single call** to the `LLMAnalysisTool` with a master prompt instructing it to perform a full analysis and generate a single JSON output.

*   **OUTPUT "STICKY NOTE" (The `next_conversation_context_package`):**
    *   **Generated By:** The `IngestionAnalyst`'s LLM call.
    *   **Stored In:** `User.next_conversation_context_package` (overwriting the previous one).
    *   **JSON Structure & Purpose:**
        ```json
        {
          "proactive_greeting": "A context-aware greeting for the start of the next conversation, e.g., 'Welcome back! Last time, we made a great plan for Project Evergreen. How are you feeling about the first step?'",
          "unresolved_topics_for_next_convo": [
            {
              "topic": "The user's relationship with their colleague, David.",
              "summary": "The user mentioned tension with David but we didn't explore it deeply.",
              "suggested_question": "If you're open to it, I'd be interested to hear more about the situation with David at work."
            }
          ],
          "suggested_initial_focus": "Start by checking in on the 'Project Evergreen' action items."
        }
        ```
    *   **Simultaneously Generated:** The LLM also generates the `persistence_payload` (memories, concepts, etc.) in the same call, which the worker uses to update the databases.

##### **C. `InsightEngine`: The Cyclical Agent**

*   **INPUT PROMPT COMPONENTS:**
    1.  `CoreIdentity` (subset): Needs to know its role as a strategic synthesizer and coach.
    2.  **Global Graph Analysis Results:** The worker first runs deterministic queries on Neo4j and PostgreSQL (e.g., concept frequency, community detection results, growth event trends).
    3.  The *previous* `userMemoryProfile`.

*   **WORKFLOW (The "Single Synthesis Call"):**
    1.  The worker is triggered for an eligible user.
    2.  It gathers all the analytical data.
    3.  It makes a **single call** to a specialized `LLMInsightTool` with a master prompt instructing it to synthesize everything and generate the complete cycle output.

*   **OUTPUT "STICKY NOTE" (The `userMemoryProfile`):**
    *   **Generated By:** The `InsightEngine`'s LLM call.
    *   **Stored In:** `User.memory_profile` (overwriting the previous cycle's profile).
    *   **JSON Structure & Purpose:** This is the most comprehensive context object.
        ```json
        {
          "profile_as_of": "ISO8601 Timestamp",
          "core_identity_summary": {
            "self_perception": ["Creative", "Anxious", "Driven"],
            "core_values": {
              "autonomy": { "salience": 0.9, "evidence": "Frequently prioritizes projects where they have creative control." },
              "community": { "salience": 0.7, "evidence": "Often mentions friends and family as a source of support." }
            }
          },
          "active_goals_and_projects": [
            { "name": "Project Evergreen", "status": "In Progress", "type": "Work Goal" },
            { "name": "Learn Guitar", "status": "Not Started", "type": "Personal Skill" }
          ],
          "emergent_themes_from_last_cycle": [
            {
              "theme": "Balancing Work and Personal Well-being",
              "related_concepts": ["Project Evergreen", "Anxiety", "Learn Guitar", "Community"]
            }
          ],
          "knowledge_graph_schema_summary": { // For the LLM's own use
             "description": "Summary of queryable nodes and relationships.",
             "concept_types": ["goal", "value", "person", "project"]
          }
        }
        ```
    *   **Simultaneously Generated:** The LLM also generates the `user_facing_artifacts` (the text for the `cycle_report` card and the questions for the `quest` cards) in the same call.

This canonical guide presents a cohesive, efficient, and deeply intelligent system. By treating agent communication as a series of "sticky notes" on a shared message board, we create a robust architecture that learns and adapts at multiple timescales, fulfilling the core vision of 2dots1line V8.5.

You are absolutely right. My previous response lacked the rigorous consistency and deep, first-person perspective of the LLM that this complex architecture demands. "Good enough" is not good enough. The system will fail if the prompts are not meticulously designed from the perspective of the "clueless LLM" that has to execute them.

I will now rectify this. I am re-calibrating my approach to be pedantically consistent and to think through every step from the LLM's point of view. I will walk in the shoes of each agent's LLM, asking "What, precisely, do I need to know to do this job perfectly, and what, precisely, must I produce?"

This is the definitive, highly-detailed, and canonical guide to the V8.5 system, designed to survive interrogation.

---

### **Definitive Canonical Guide: The V8.5 Multi-Layered State & Prompting Architecture**

#### **Section 1: Terminology and Data Model Lock-in**

First, let's establish an unbreakable contract on terminology to eliminate all confusion.

| Formal Term in This Document       | Corresponds to Database Field                | Description                                                                                                                              |
| :--------------------------------- | :------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------- |
| **`CoreIdentity`**                 | (Loaded from `CoreIdentity.yaml` into Redis) | The global, static persona of the AI assistant, "Dot".                                                                                   |
| **`UserMemoryProfile`**            | `User.memory_profile` (JSONB)                | **The Strategic Context.** A comprehensive summary of the user's entire graph, generated by the `InsightEngine` during a cycle.          |
| **`NextConversationContextPackage`** | `User.next_conversation_context_package` (JSONB) | **The Inter-Conversational Context.** A "debrief memo" with proactive starters for the *next* conversation, generated by the `IngestionAnalyst`. |
| **`TurnContextPackage`**           | `Conversation.turn_context_package` (JSONB)  | **The Tactical Context.** A "sticky note" with focus and tone for the *next turn*, generated by the `DialogueAgent`.                   |

This terminology will be used with 100% consistency henceforth.

---

#### **Section 2: The LLM's Perspective - Task Dependencies & Required Context**

Let's walk in the shoes of the LLM for each agent, defining exactly what it needs to know to perform its task without ambiguity.

##### **Agent 1: The `DialogueAgent` LLM**

*   **My Job:** I am in a live conversation. My goal is to give a single, relevant, and helpful response to the user's immediate message.
*   **My Core Problem:** The user might mention something ("Project Phoenix") that isn't in my immediate conversational memory. I need to know if I should ask for more information before responding. If I do need more info, I need to know *how to ask for it*.
*   **The Solution (My Required Context):**
    1.  **I need to know my own identity:** The `CoreIdentity` tells me who I am and my fundamental rules.
    2.  **I need a summary of the user's entire world:** The `UserMemoryProfile` gives me the long-term strategic overview.
    3.  **I need to know how the last conversation ended:** The `NextConversationContextPackage` gives me a proactive way to start this new conversation. (For first turn only).
    4.  **I need to know what we were *just* talking about:** The `TurnContextPackage` reminds me of my focus from the last turn. (For all subsequent turns).
    5.  **I need to know *how to ask for more information*.** This is the critical missing piece. I cannot "invent" a Cypher query. I need a **"map" of the knowledge graph**.

*   **Solution Implementation: The `KnowledgeGraphSchema` Component**
    *   This is the "map" the LLM needs. It will be a component **within** the `UserMemoryProfile` and generated by the `InsightEngine`. It is the answer to "how is the LLM supposed to know what data is available".

    *   **Revised `UserMemoryProfile` Structure:**
        ```json
        {
          "profile_as_of": "...",
          "core_identity_summary": { ... },
          "active_goals_and_projects": [ ... ],
          "emergent_themes_from_last_cycle": [ ... ],
          "knowledge_graph_schema": {
            "description": "This is a map of the user's queryable knowledge. Use it to construct Cypher queries when you need more information.",
            "node_labels_and_properties": {
              "MemoryUnit": ["muid", "title", "content", "creation_ts"],
              "Concept": ["id", "name", "type", "description"],
              "Community": ["community_id", "name"]
            },
            "relationship_types_and_properties": {
              "HIGHLIGHTS": ["weight"],
              "RELATED_TO": ["relationship_label", "weight"],
              "BELONGS_TO": []
            },
            "example_queries": [
              {
                "goal": "Find all memories related to a specific concept name.",
                "query": "MATCH (c:Concept {name: $conceptName})<-[:HIGHLIGHTS]-(m:MemoryUnit) RETURN m.title, m.content"
              },
              {
                "goal": "Find concepts related to another concept.",
                "query": "MATCH (c1:Concept {name: $conceptName})-[:RELATED_TO]->(c2:Concept) RETURN c2.name, c2.type"
              }
            ]
          }
        }
        ```
    *   **Result:** The LLM is no longer "clueless". It is explicitly told: "Here is a summary of what we know. If the user mentions something not in this summary, use this provided schema and these example queries to build a valid Cypher query to get the details."

##### **Agent 2: The `IngestionAnalyst` LLM**

*   **My Job:** I have just been given a complete, raw transcript of a conversation. My goal is to extract all important information and prepare the system for the user's *next* conversation.
*   **My Core Problem:** I need to understand what the user *already* knows so I don't create duplicate concepts. I also need to understand the flow of the conversation to identify growth moments.
*   **The Solution (My Required Context):**
    1.  **I need the full, raw transcript:** This is my primary data.
    2.  **I need the `UserMemoryProfile`:** This is my "database" of existing knowledge. When I extract a concept like "Project Phoenix," I must first check if a concept with a similar name or description already exists in the profile before deciding to create a new one. This prevents duplication.
    3.  **I need the `TurnContextPackage` from every turn:** The `flags_for_ingestion` array (e.g., `"high_emotional_intensity"`) within each turn's package gives me critical clues about which parts of the conversation were most significant, helping me assign a more accurate `conversation_importance_score`.

##### **Agent 3: The `InsightEngine` LLM**

*   **My Job:** I am a strategist. I have been given a high-level summary of all the data changes that occurred during the last cycle. My job is to update the user's strategic plan (`UserMemoryProfile`) and identify future growth opportunities ("Quests").
*   **My Core Problem:** I need to know what worked and what didn't in the past. I also need to avoid suggesting the same things over and over.
*   **The Solution (My Required Context):**
    1.  **I need a summary of all new entities and events from the cycle:** The worker provides this by querying the database for all items created since `last_cycle_started_at`.
    2.  **I need the *previous* `UserMemoryProfile`:** I need to see the user's state *before* this cycle so I can understand what has changed and generate a meaningful `cycle_report`.
    3.  **I need a history of past "Quests":** I must not suggest the same `ProactivePrompt`s repeatedly. The worker will provide me with a list of recently created `ProactivePrompt`s so I can ensure my new suggestions are novel.
    4.  **I need a history of effective query patterns:** This is the "message to my future self" you mentioned. The `InsightEngine` worker will analyze the `DialogueAgent`'s successful `query_memory` decisions from the last cycle and feed this into the prompt.

*   **Solution Implementation: The `EffectiveQueryPatterns` Component**
    *   This component will be added to the prompt given to the `InsightEngine` LLM.
    *   The `InsightEngine` worker is responsible for generating this by analyzing past conversations.
    *   **Example `EffectiveQueryPatterns` structure:**
        ```json
        {
          "most_frequent_node_types_queried": ["Concept", "MemoryUnit"],
          "most_frequent_concept_types_queried": ["goal", "person"],
          "high_yield_query_structures": [
            "Finding memories related to a specific person concept.",
            "Tracing the causes and effects between two project concepts."
          ]
        }
        ```
    *   **Result:** The `InsightEngine` is now explicitly told, "For this user, these types of queries have been very helpful recently. Use this knowledge to generate a more effective `knowledge_graph_schema` and more relevant `ProactivePrompt`s for the next cycle."

---

#### **Section 3: The Holistic Prompt Packing List & Output Checklist**

This is the definitive, step-by-step breakdown of what each agent's LLM call receives and produces.

##### **A. `DialogueAgent` LLM Call**

*   **Input Context Packing List:**
    1.  `CoreIdentity`: The static `yaml` file content.
    2.  `UserMemoryProfile`: The full JSON object from `User.memory_profile`, including the crucial `knowledge_graph_schema` sub-object.
    3.  `NextConversationContextPackage` (if first turn): The full JSON object from `User.next_conversation_context_package`.
    4.  `TurnContextPackage` (if not first turn): The full JSON object from `Conversation.turn_context_package`.
    5.  `CurrentConversationHistory`: A formatted transcript of the last ~10 messages.

*   **Output Checklist (A Single JSON Object):**
    1.  `thought_process`: A string explaining the agent's reasoning.
    2.  `response_plan`: An object containing:
        *   `decision`: The string `'respond_directly'` or `'query_memory'`.
        *   `cypher_query`: A valid Cypher query string if decision is `query_memory`, otherwise `null`.
        *   `direct_response_text`: The user-facing response if decision is `respond_directly`, otherwise `null`.
    3.  `turn_context_package`: A new JSON object with `summary_of_last_turn`, `suggested_next_focus`, `emotional_tone_to_adopt`, and `flags_for_ingestion`.

##### **B. `IngestionAnalyst` LLM Call**

*   **Input Context Packing List:**
    1.  `CoreIdentity` (Analyst Subset): A prompt defining its role as a knowledge extractor.
    2.  `UserMemoryProfile`: The full JSON object, used to check for existing concepts.
    3.  `FullConversationTranscript`: The complete transcript of the ended conversation.
    4.  `AccumulatedTurnFlags`: A list of all `flags_for_ingestion` collected from each `TurnContextPackage` during the conversation.

*   **Output Checklist (A Single JSON Object):**
    1.  `persistence_payload`: An object containing:
        *   `conversation_summary` (string)
        *   `conversation_importance_score` (number)
        *   `extracted_memory_units` (array of objects)
        *   `extracted_concepts` (array of objects)
        *   `detected_growth_events` (array of objects, **each including a `rationale` string**)
        *   `new_relationships` (array of objects)
    2.  `forward_looking_context` (The `NextConversationContextPackage`): An object containing:
        *   `proactive_greeting` (string)
        *   `unresolved_topics_for_next_convo` (array of objects)
        *   `suggested_initial_focus` (string)

##### **C. `InsightEngine` LLM Call**

*   **Input Context Packing List:**
    1.  `CoreIdentity` (Strategist Subset): A prompt defining its role as a strategic synthesizer.
    2.  `PreviousUserMemoryProfile`: The user's `memory_profile` from the *start* of the cycle.
    3.  `CycleActivitySummary`: A structured summary generated by the worker, including lists of new concepts, memory units, and growth events from the cycle.
    4.  `RecentQuestHistory`: A list of the last N `ProactivePrompt`s created for the user.
    5.  `EffectiveQueryPatterns`: The structured JSON object detailing what kinds of queries have worked well for this user.

*   **Output Checklist (A Single JSON Object):**
    1.  `persistence_payload`: An object containing:
        *   `ontology_merges_needed`: Instructions for merging concepts.
    2.  `user_facing_artifacts`: An object containing:
        *   `cycle_report_card`: The full text content for the user's cycle report.
        *   `quest_card_prompts`: An array of new `ProactivePrompt` questions.
    3.  `forward_looking_context`: An object containing:
        *   `updated_user_memory_profile`: The complete, new `UserMemoryProfile` JSON object, including a regenerated and potentially improved `knowledge_graph_schema`.

This meticulously detailed and holistic design addresses the previous ambiguities. The LLM is now treated as a powerful but "clueless" engine that is given a perfect, complete set of instructions and context for every task. It knows *how* to query the graph because we give it a map and examples. It knows *what* to suggest because we tell it what has worked before. It knows *what* to write because we give it a complete picture of the user. This architecture is now robust enough to build upon.