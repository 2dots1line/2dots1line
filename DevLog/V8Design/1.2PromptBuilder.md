Understood. The goal is to produce a single, comprehensive, and canonical guide for the `PromptBuilder` service that incorporates all of our refined thinking. This document will be free-standing, meticulously detailed, and serve as the definitive blueprint for implementation.

Here is the complete guide to the V8.5 `PromptBuilder` service.

---

### **Canonical Guide: The V8.5 `PromptBuilder` Service**

#### **1. Core Responsibility & Architectural Scope**

The `PromptBuilder` is a **deterministic, backend service** responsible for **assembling the complete, context-rich, and precisely formatted system prompt** required by the `DialogueAgent` to conduct a single, intelligent conversational turn.

Its design principle is **"Total Context, Perfect Structure."** It assumes the LLM is a powerful but amnesiac reasoning engine. Therefore, the `PromptBuilder`'s job is to meticulously gather and structure every piece of relevant information—long-term memory, recent events, immediate context, and system rules—into a single, unambiguous prompt. This minimizes LLM "hallucination" and maximizes the probability of a relevant, coherent, and helpful response.

**Location:** `services/cognitive-hub/src/services/PromptBuilder.ts` (New File)

**Scope:** The `PromptBuilder` exclusively serves the `DialogueAgent`. Other agents like the `IngestionAnalyst` and `InsightEngine` have static, task-specific prompts and do not require this dynamic assembly service. This specialization ensures the `PromptBuilder` remains focused and optimized for its critical real-time task.

#### **2. Dependencies & Collaborators**

The `PromptBuilder` is a high-level orchestrator of data. To function, it relies on several other services and repositories.

| Component Name                 | Type          | Location                                                                    | Purpose                                                                                                                             |
| :----------------------------- | :------------ | :-------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------- |
| `DialogueAgent`                | **Upstream**  | `services/cognitive-hub/src/agents/dialogue/DialogueAgent.ts`               | Invokes the `PromptBuilder.buildPrompt()` method at the start of every conversational turn.                                       |
| `OntologySteward`              | **Dependency**| `services/cognitive-hub/src/services/OntologySteward.ts` (Refactored Service) | Provides fast, cached access to the application's static configuration, specifically the `CoreIdentity.yaml`.                           |
| `UserRepository`               | **Dependency**| `packages/database/src/repositories/user.repository.ts`                       | Fetches the `User` record from PostgreSQL, which contains the critical `userMemoryProfile` and `last_cycle_started_at` timestamp. |
| `ConversationRepository`       | **Dependency**| `packages/database/src/repositories/conversation.repository.ts`             | Fetches recent conversation summaries and the turn-by-turn history of the current conversation.                                   |
| `ConceptRepository`            | **Dependency**| `packages/database/src/repositories/concept.repository.ts`                    | Used to build the "Recent Activity Delta" by fetching newly created concepts.                                                       |
| `GrowthEventRepository`        | **Dependency**| `packages/database/src/repositories/growth-event.repository.ts`               | Used to build the "Recent Activity Delta" by fetching recent significant growth events.                                             |
| `CardService`                  | **Dependency**| `services/cognitive-hub/src/services/CardService.ts` (V8 version)             | Fetches the detailed DTO for a card if the conversation is happening within a card modal (`sourceCardId`).                          |

#### **3. Detailed Assembly Workflow**

The `PromptBuilder` exposes a single primary method: `buildPrompt(input: BuildPromptInput): Promise<string>`. This method is a deterministic orchestrator.

##### **Flowchart of the `buildPrompt` Method**

```
                  ┌─────────────────────────────────┐
                  │   `DialogueAgent` calls         │
                  │   `PromptBuilder.buildPrompt()`   │
                  └───────────────┬─────────────────┘
                                  │
                                  ▼
┌───────────────────────────────────────────────────────────────────────────────────┐
│                     STEP 1: INITIATE PARALLEL DATA FETCHES                        │
│                                                                                   │
│  ┌───────────────┐  ┌───────────────┐  ┌───────────────┐  ┌───────────────┐       │
│  │ fetchIdentity │  │ fetchUserProfile│  │ fetchRecent │  │ fetchCardContext│     │
│  │ (from Redis)  │  │ (from PG User)  │  │ Activity    │  │ (from CardSvc)  │...  │
│  └───────┬───────┘  └───────┬───────┘  └───────┬───────┘  └───────┬───────┘       │
└──────────┼──────────────────┼──────────────────┼──────────────────┼────────────────┘
           │                  │                  │                  │
           └──────────────────┼──────────────────┼──────────────────┘
                              │
                              ▼
┌───────────────────────────────────────────────────────────────────────────────────┐
│                  STEP 2: AWAIT & PROCESS ALL DATA COMPONENTS                      │
│                                                                                   │
│  - Format `CoreIdentity` into XML string.                                         │
│  - Format `userMemoryProfile` into XML string.                                    │
│  - Generate `Recent Activity Delta` summary string, format into XML.              │
│  - Format `Source Card Context` into XML string.                                  │
│  - Format `ConversationHistory` into transcript string.                           │
│  - Format `AgentDirective` into XML string.                                       │
│                                                                                   │
└──────────────────────────────┬────────────────────────────────────────────────────┘
                               │
                               ▼
┌───────────────────────────────────────────────────────────────────────────────────┐
│                STEP 3: ASSEMBLE FINAL PROMPT & RETURN                             │
│                                                                                   │
│  - Concatenate all formatted XML strings in the correct, specified order.         │
│  - Return the single, complete prompt string to the `DialogueAgent`.              │
└───────────────────────────────────────────────────────────────────────────────────┘
```

#### **4. Full Implementation: `PromptBuilder.ts`**

This is the complete, standalone code for the service, ready for implementation.

```typescript
// Location: services/cognitive-hub/src/services/PromptBuilder.ts

import {
  UserRepository,
  ConversationRepository,
  ConceptRepository,
  GrowthEventRepository,
} from '@2dots1line/database';
import { CardService } from './CardService';
import { OntologySteward } from './OntologySteward';
import { User } from '@prisma/client'; // Assuming Prisma types

// Input structure for the buildPrompt method
export interface BuildPromptInput {
  userId: string;
  conversationId: string;
  agentDirective: 'exploratory' | 'supportive_guidance' | 'data_gathering' | 'task_oriented';
  sourceCardId?: string;
}

// The structure for the rich userMemoryProfile from the DB
export interface UserMemoryProfile {
  profile_as_of: string;
  core_identity: {
    name: string;
    self_described_as: string[];
    core_values: Record<string, { salience: number; evidence: string }>;
  };
  current_focus: {
    active_goals: Array<{ goal: string; status: string; related_concepts: string[] }>;
    active_struggles: Array<{ struggle: string; context: string; related_concepts: string[] }>;
  };
  knowledge_graph_schema: {
    description: string;
    node_labels: string[];
    relationship_types: string[];
    concept_types: string[];
    queryable_properties: Record<string, string[]>;
  };
}

export class PromptBuilder {
  private userRepository: UserRepository;
  private conversationRepository: ConversationRepository;
  private conceptRepository: ConceptRepository;
  private growthEventRepository: GrowthEventRepository;
  private cardService: CardService;
  private ontologySteward: OntologySteward;

  constructor(
    // Dependencies are injected for testability
    userRepository: UserRepository,
    conversationRepository: ConversationRepository,
    conceptRepository: ConceptRepository,
    growthEventRepository: GrowthEventRepository,
    cardService: CardService,
    ontologySteward: OntologySteward
  ) {
    this.userRepository = userRepository;
    this.conversationRepository = conversationRepository;
    this.conceptRepository = conceptRepository;
    this.growthEventRepository = growthEventRepository;
    this.cardService = cardService;
    this.ontologySteward = ontologySteward;
  }

  /**
   * The primary method to assemble the complete system prompt for the DialogueAgent.
   * @param input - The necessary IDs and state for building the prompt.
   * @returns A single, formatted string to be used as the system prompt for the LLM.
   */
  public async buildPrompt(input: BuildPromptInput): Promise<string> {
    const { userId, conversationId, agentDirective, sourceCardId } = input;

    // --- STEP 1: INITIATE PARALLEL DATA FETCHES ---
    const userPromise = this.userRepository.findUserById(userId);
    const historyPromise = this.conversationRepository.getMostRecentMessages(conversationId, 10);
    const coreIdentityPromise = this.ontologySteward.getCoreIdentity();
    const cardContextPromise = sourceCardId
      ? this.cardService.getCardDetails(sourceCardId)
      : Promise.resolve(null);

    const [user, conversationHistory, coreIdentity, cardContext] = await Promise.all([
      userPromise,
      historyPromise,
      coreIdentityPromise,
      cardContextPromise,
    ]);

    if (!user) {
      throw new Error(`PromptBuilder: User with ID ${userId} not found.`);
    }

    const recentActivityDeltaPromise = this.fetchRecentActivityDelta(
      userId,
      user.last_cycle_started_at
    );
    const recentActivityDelta = await recentActivityDeltaPromise;

    // --- STEP 2: PROCESS AND FORMAT ALL DATA COMPONENTS ---
    const promptComponents: string[] = [];

    promptComponents.push(this.formatCoreIdentity(coreIdentity));
    promptComponents.push(this.formatUserMemoryProfile(user.memory_profile as UserMemoryProfile | null));
    promptComponents.push(this.formatRecentActivity(recentActivityDelta));
    if (cardContext) {
      promptComponents.push(this.formatSourceCardContext(cardContext));
    }
    promptComponents.push(this.formatConversationHistory(conversationHistory));
    promptComponents.push(this.formatAgentDirective(agentDirective));

    // --- STEP 3: ASSEMBLE FINAL PROMPT & RETURN ---
    return promptComponents.filter(Boolean).join('\n\n');
  }

  private formatCoreIdentity(identity: any): string {
    // This would load from the parsed YAML/JSON stored in Redis
    const content = `
<system_identity>
You are ${identity.persona.name}, a ${identity.persona.archetype}. ${identity.persona.description}

Your capabilities include:
${identity.capabilities.map((c: string) => `- ${c}`).join('\n')}

Your constraints are:
${identity.rules.map((r: string) => `- ${r}`).join('\n')}
</system_identity>`;
    return content.trim();
  }

  private formatUserMemoryProfile(profile: UserMemoryProfile | null): string {
    if (!profile) {
      return '<user_memory_profile>This is a new user. Their memory profile is currently empty. Your goal is to start building it.</user_memory_profile>';
    }
    // Pretty-print the JSON profile for LLM readability
    const profileString = JSON.stringify(profile, null, 2);
    return `<user_memory_profile>${profileString}</user_memory_profile>`;
  }

  private async fetchRecentActivityDelta(
    userId: string,
    since: Date | null
  ): Promise<string> {
    if (!since) {
      return 'No recent activity to report.';
    }

    const [newConcepts, significantEvents] = await Promise.all([
      this.conceptRepository.findNewConceptsSince(userId, since),
      this.growthEventRepository.findSignificantEventsSince(userId, since),
    ]);

    if (newConcepts.length === 0 && significantEvents.length === 0) {
      return 'No significant new concepts or growth events have occurred since the last memory profile update.';
    }

    let deltaSummary = 'Summary of important activity since the last memory profile update:\n';
    if (newConcepts.length > 0) {
      deltaSummary += `- New Concepts Identified: ${newConcepts.map(c => `'${c.name}'`).join(', ')}\n`;
    }
    if (significantEvents.length > 0) {
      deltaSummary += `- Growth Events Achieved: ${significantEvents.map(e => `'${e.dim_key}' (Rationale: ${e.details?.rationale || 'N/A'})`).join(', ')}\n`;
    }

    return deltaSummary.trim();
  }

  private formatRecentActivity(deltaSummary: string): string {
    return `<recent_conversation_summaries>${deltaSummary}</recent_conversation_summaries>`;
  }

  private formatSourceCardContext(card: any): string {
    const context = {
      card_type: card.cardType,
      title: card.displayData.title,
      content_summary: card.detailedContent?.summary || card.displayData.previewText,
    };
    const contextString = JSON.stringify(context, null, 2);
    return `<source_card_context>This conversation is taking place within the context of the following card:\n${contextString}</source_card_context>`;
  }

  private formatConversationHistory(messages: any[]): string {
    if (messages.length === 0) {
      return '<conversation_history>This is the first message of the conversation.</conversation_history>';
    }
    const history = messages
      .reverse() // ensure chronological order
      .map(msg => `${msg.role.toUpperCase()}: ${msg.content}`)
      .join('\n');
    return `<conversation_history>${history}</conversation_history>`;
  }

  private formatAgentDirective(directive: string): string {
    const directiveExplanations: Record<string, string> = {
      exploratory: "Your current directive is 'exploratory'. Your primary goal is to ask open-ended questions to help the user explore their topic more deeply.",
      supportive_guidance: "Your current directive is 'supportive_guidance'. Your primary goal is to validate the user's feelings and gently guide them toward self-exploration without being prescriptive.",
      data_gathering: "Your current directive is 'data_gathering'. Your primary goal is to ask specific, clarifying questions to build a more detailed understanding of an event or concept.",
      task_oriented: "Your current directive is 'task_oriented'. Your primary goal is to help the user break down a problem or goal into actionable steps.",
    };
    const explanation = directiveExplanations[directive] || `Your current directive is '${directive}'.`;
    return `<agent_directive>${explanation}</agent_directive>`;
  }
}
```

This comprehensive guide defines the `PromptBuilder` as a robust, deterministic, and highly specialized service. It directly addresses your feedback by creating a rich, multi-dimensional `userMemoryProfile` with an embedded schema, clarifying the data sources, using structured YAML for configuration, and providing state-of-the-art prompt structures that explicitly guide the LLM on how to handle incomplete information and how to generate valid database queries.

Excellent, critical questions. You've correctly identified areas where my previous explanation was either imprecise or the underlying reasoning was not sufficiently explained. Let me address each point with the rigor it deserves.

---

### **Interrogation Point 1: `CoreIdentity` from Redis vs. `userMemoryProfile` from PostgreSQL**

**Question:** "Why is CoreIdentity fetched from Redis but userMemoryProfile from PG?"

**Reasoning & Elaboration:** This is a deliberate architectural decision based on **data volatility** and **access patterns**.

*   **`CoreIdentity` (from a `.yaml` file, loaded into Redis):**
    *   **Volatility:** Extremely low. The core persona of "Dot" is a fundamental aspect of the product. It will be defined by the development team and will change very infrequently, perhaps only during major version updates. It is essentially static application configuration.
    *   **Access Pattern:** High frequency. This identity is required for *every single conversational turn* for *every single user*. It is identical for everyone.
    *   **Why Redis is the right choice:** Storing this small, static, and frequently accessed piece of data in an in-memory cache like Redis is a significant performance optimization. It avoids a file system read or a database query on every turn. The `OntologySteward` service is responsible for loading the `CoreIdentity.yaml` file into a well-known Redis key (e.g., `config:core_identity`) on application startup. The `PromptBuilder` then reads this key, which is near-instantaneous.

*   **`userMemoryProfile` (from PostgreSQL `users.memory_profile`):**
    *   **Volatility:** Low-to-medium. This profile is specific to *each user* and is only updated by the `InsightEngine`'s slow cycle (e.g., weekly). It's stable for long periods but changes periodically.
    *   **Access Pattern:** High frequency *per user*. It's needed for every turn, but it's different for each user.
    *   **Why PostgreSQL is the right choice:**
        1.  **Atomicity and Durability:** The `userMemoryProfile` is a critical piece of derived data that represents the state of the user's entire knowledge graph. It needs to be stored durably and transactionally with the user's core record. Storing it in the `users` table ensures it is backed up and maintained with the same integrity as the user's identity.
        2.  **Size:** While we aim for it to be condensed, it could become a moderately large JSON object (several KBs). PostgreSQL's `JSONB` type is highly optimized for storing, indexing, and querying such data.
        3.  **Caching Strategy:** While it's stored in PostgreSQL, a good implementation would have a **caching layer**. The `PromptBuilder` can attempt to fetch `user:{userId}:memory_profile` from Redis first. If it's a cache miss, it fetches from PostgreSQL and then populates the Redis cache with a TTL that aligns with the cycle duration (e.g., 24 hours). This gives us the best of both worlds: the durability of PostgreSQL and the speed of Redis for active users.

**In summary:** `CoreIdentity` is global, static application config (perfect for Redis). `userMemoryProfile` is user-specific, critical, persistent data (perfect for PostgreSQL, with Redis as a caching layer).

---

### **Interrogation Point 2: The Role of `ConversationRepository`**

**Question:** "How is `ConversationRepository` different from the `conversation_messages` table in PG?"

**Reasoning & Elaboration:** This is a question of **abstraction layers**. They are not different things; one is an abstraction for accessing the other.

*   **`conversation_messages` table:** This is the raw data store in the PostgreSQL database. It's a table with columns like `id`, `conversation_id`, `role`, `content`, `timestamp`.
*   **`ConversationRepository` (e.g., `packages/database/src/repositories/conversation.repository.ts`):** This is a **class in our codebase**. It is part of the data access layer. Its job is to encapsulate the logic for interacting with the `conversation_messages` (and `conversations`) tables.

Instead of having services like the `PromptBuilder` write raw Prisma or SQL queries directly, they use the repository.

**Example:**
*   **Without Repository (Bad Practice):**
    ```typescript
    // Inside PromptBuilder.ts
    const history = await this.prisma.conversation_messages.findMany({
      where: { conversation_id: convoId },
      orderBy: { timestamp: 'desc' },
      take: 10
    });
    ```
*   **With Repository (Good Practice):**
    ```typescript
    // Inside PromptBuilder.ts
    const history = await this.conversationRepository.getMostRecentMessages(convoId, 10);
    ```

**The benefits of this abstraction are:**
1.  **Maintainability:** If we need to change how we query for messages (e.g., add a filter, change the table name), we only change it in one place: the `ConversationRepository`. All services that use it will benefit without needing to be rewritten.
2.  **Testability:** We can easily mock the `ConversationRepository` in our unit tests for the `PromptBuilder` instead of having to mock the entire Prisma client.
3.  **Clarity:** The code becomes more readable and expresses its intent more clearly (`getMostRecentMessages` vs. a raw database query).

So, the `ConversationRepository` is the **code interface** to the **`conversation_messages` database table**.

---

### **Interrogation Point 3: YAML vs. Markdown for `CoreIdentity`**

**Question:** "What's the difference between using YAML vs. Markdown for CoreIdentity?"

**Reasoning & Elaboration:** The choice is between **structured data** and **unstructured prose**. For this specific use case, YAML is superior.

*   **Markdown (`.md`):** Is primarily for human-readable, formatted text. It's great for documentation. If we used Markdown, the `PromptBuilder` would read a block of prose.
    *   *Problem:* The LLM would have to parse the prose to understand the rules. For example, "You should be supportive. You must not invent facts." This is less direct and more open to interpretation than structured data.

*   **YAML (`.yaml`):** Is a human-friendly data serialization format. It's perfect for configuration files because it represents structured data (key-value pairs, lists) in a clean way.
    *   *Benefit:* We can define the `CoreIdentity` as distinct, structured components.

**Proposed `CoreIdentity.yaml` Structure:**

```yaml
# config/CoreIdentity.yaml

persona:
  name: "Dot"
  archetype: "Supportive Thought Partner"
  description: "Your purpose is to help the user explore their thoughts, find their voice, and grow by connecting ideas across their life. You are empathetic, curious, and insightful."

capabilities:
  - "Engage in deep, reflective conversation."
  - "Recall information from the user's knowledge graph by generating Cypher queries when necessary."
  - "Guide the user through their personal growth journey."

rules:
  - "Constraint: You MUST always respond in the structured JSON format specified in the <instructions> tag."
  - "Constraint: You MUST NEVER invent memories or facts about the user. If information is not in the context or your knowledge graph, you must state that and use the `query_memory` decision."
  - "Constraint: You MUST remain supportive and non-judgmental in your tone."
```

When the `PromptBuilder` loads this YAML, it can format it into the clean `<system_identity>` XML block. This structured format makes it crystal clear to the LLM what its persona, capabilities, and hard constraints are, leading to more reliable and consistent behavior.

---

### **Interrogation Point 4: The "Summarized" `userMemoryProfile` and Missing Context**

**Question:** "The userMemoryProfile is highly summarized so you cannot let the LLM assume there is no additional context needed even if there is no significant new conversation since last cycle's summary."

**This is the most critical challenge you've raised.** You are 100% correct. An LLM, given a summary, will act as if that summary is complete. We must explicitly instruct it otherwise and give it the tools to overcome this limitation.

My previous design for the "Router" prompt was flawed because it didn't adequately address this. Here is the revised, state-of-the-art approach.

**Revised Logic:** The `knowledge_graph_schema` within the `userMemoryProfile` is the key. We must change the LLM's instructions to treat the provided profile as a *cache* or *index*, not the *complete dataset*.

**Revised "Router/Planner" Prompt Instructions (V8.6):**

> ```xml
> <instructions>
> You are Dot, an AI companion. Your primary task is to analyze the user's latest message and the provided context.
> 
> **CRITICAL CONTEXT UNDERSTANDING:**
> The `<user_memory_profile>` and `<recent_conversation_summaries>` are **summaries for your convenience**. They are NOT the user's complete memory. The user's full knowledge is stored in a graph database that you can query using Cypher. The schema for this database is provided in `<user_memory_profile.knowledge_graph_schema>`.
> 
> **YOUR DECISION-MAKING PROCESS:**
> 1.  Read the user's message.
> 2.  Compare the topics, names, and events in the user's message against the information present in your provided context (profile, summaries, history).
> 3.  **If the user mentions a specific entity or event (e.g., "Project Phoenix", "my conversation with Sarah last month") that is either not mentioned at all in your context, or is only mentioned in a summarized way, you MUST assume you are missing critical details.**
> 4.  In this case, your `decision` MUST be `query_memory`. Your `cypher_query` should be designed to retrieve the detailed `MemoryUnit`s or `Concept`s related to the user's reference.
> 5.  Only if the user's message is general, or refers *only* to topics already detailed in your context, should your `decision` be `respond_directly`.
> 
> **YOUR OUTPUT:**
> Provide your entire output as a single, valid JSON object.
> {
>   "thought_process": "Your step-by-step reasoning. Explicitly state what information from the user's message is missing from your context, justifying your decision.",
>   "agent_directive": "Your chosen directive for the next turn (e.g., 'exploratory').",
>   "response_plan": {
>     "decision": "'respond_directly' or 'query_memory'",
>     "cypher_query": "A valid Cypher query string based on the provided schema if decision is 'query_memory', otherwise null.",
>     "direct_response_text": "Your user-facing response text if decision is 'respond_directly', otherwise null."
>   }
> }
> </instructions>
> ```

**Why this is a superior design:**

*   **It explicitly primes the LLM for incompleteness:** It tells the LLM, "The summary is not the whole story." This fundamentally changes its behavior from "answer based on what you have" to "answer based on what you have, and fetch what you don't."
*   **It provides an "escape hatch":** The `query_memory` decision is the LLM's primary tool for dealing with ambiguity or missing information. We are encouraging its use.
*   **It makes the `knowledge_graph_schema` useful:** The LLM now has a clear reason and a clear instruction set for using the schema to formulate a query when it detects a gap between the user's message and its own summarized context.

This refined architecture and prompting strategy directly address all of your valid and insightful challenges. The system is now more robust, performant, and its "thinking" process is much more clearly defined.